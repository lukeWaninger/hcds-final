{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Source of Wildfires\n",
    "Luke Waninger\n",
    "\n",
    "DATA 512 Final Project\n",
    "\n",
    "University of Washington, Fall 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wildfires have been a big topic in the recent news with devasting effects across the western coast of the United States. So far this year, we have had less burn than 2017, but the current fire in California is the largest in state history and still burns rapidly. Last year, we had almost 2 billion dollars of losses across the United States as a result of wildfire damage which has been the highest in history. Risks of wildfires continue to climb as scientists discover alarming links between rising greenhouse gasses, temperature, and wildfire severity. N. P. Gillett et al. performed a comprehensive study on the relationship between the two and concluded with overwhelming confidence that a positive trend exists between them. Rising greenhouse gasses could be playing a significant role in the prevalence and severity of forest fires.\n",
    "\n",
    "Key to understanding the overall problem is the double-edged sword forests play in climate change; they are both a cause and effect. The wildfires both increase atmospheric greenhouse gasses and destroy the integral vegetation to the planet's carbon cycle. The Paris Agreement has specifically mentioned the importance of this and insists that countries protect against deforestation. Not only is the world pushing to keep the forests we have but here at home, we have begun to employ them as significant combatants in the fight against climate change. California has led the way with their proposed carbon plan. It proposes methods to reshape parts of their existing ecosystem to make their forests even more efficient at removing carbon. Stopping deforestation would significantly promote the UNs progress towards reaching goals outlined in the Paris Agreement.\n",
    "\n",
    "However, this will not work if the forests continue in the same destructive cycle with our ecosystem. The goal of this project is two-fold. One, to understand the independent variables and correlation effects in a combined dataset of the Fire Program Analysis (FPA) reporting system, NOAA's Global Surface Summary of Day Data (GSOD) 7, and  NASA's biomass indicators. Two, to train and assess a model for predicting the reason a wildfire started. (and possibly estimate the impact? location?) Identifying the source is a difficult task for investigators in the wild. The vastness of land covered is much larger than the matchstick or location of a lightning strike. Developing an understanding of the independent variables and a reliable prediction model could give authorities valuable direction as to where to begin their search.\n",
    "\n",
    "#### Research Questions\n",
    "* What are the most important indicators to consider when determining the strength of a wildfire?\n",
    "* Can a reliable model be built to assist investigators in determining the cause of a wildfire?\n",
    "\n",
    "####  Reproducibility\n",
    "This notebook is intended to be completely reproducible. However, the final dataset is much too large to be hosted on GitHub. I provide a small, randomly selected sample with the repository to show the dataset cleaning and generation process. The full dataset can be downloaded by running the following cell. Running it will overwrite the sample provided. To undoe this effect, reclone the repository. I highly advise not to attempting to run the full dataset. Generating the weather aggregations across the entire fires dataset can take a considerable amount of time. With 12 cores running at 4ghz and a consistent 95% CPU load, it took my machine nearly 27 hours to compute.\n",
    "\n",
    "Additionally, this notebook is run as two major parts. Part 1 consists of data cleaning and final dataset creation. This can be run any machine with at least 20gb of memory. The seconds portion is analysis which requires a much larger system to compute. Full directions are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "This notebook is coded to run with Python 3.6. Several libraries from the Python standard library will be used along with several third-party modules. These can be installed with the provided requirements file using the command \n",
    "\n",
    "`pip install --user -r requirements.txt`\n",
    "\n",
    "More information regarding the standard libarary can be found at [python.org](https://docs.python.org/3.6/library/index.html).\n",
    "\n",
    "For third party libraries please see:\n",
    "* [numpy == 1.13.0](https://docs.scipy.org/doc/numpy-1.13.0/reference/)\n",
    "* [pandas == 0.23.4](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "* [plotly == 3.4.2](https://plot.ly/python/)\n",
    "* [statsmodels == 0.9.0](https://www.statsmodels.org/stable/index.html)\n",
    "* [tqdm == 4.28.1](https://github.com/tqdm/tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T17:04:22.767613Z",
     "start_time": "2018-12-05T17:04:22.749454Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Python standard library\n",
    "import datetime as dt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import IFrame\n",
    "import itertools as it\n",
    "import multiprocessing as mul\n",
    "from multiprocessing.dummy import Pool as TPool\n",
    "import os\n",
    "import gzip\n",
    "import sqlite3\n",
    "import sys\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "# third party modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import plotly.figure_factory as ff\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "# initialize plotly    \n",
    "#init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "# set notebook options\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# initalize tqdm\n",
    "tqdm.pandas(leave=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Sources\n",
    "Four data sources are to be used for this project. The primary data source was found through Kaggle and contains 1.88 million wildfires that occurred in the United States from 1992 to 2015. This data contains the primary labels to be used as target variables. The United States Department of Agriculture curated the original data ([Forest Service](https://www.fs.fed.us/)) and can be found at [link](https://www.fs.usda.gov/rds/archive/Product/RDS-2013-0009.4/). The second is the GSOD data curated by [NOAA](https://www.noaa.gov/). Finally, the National Air and Space Association (NASA) hosts a valuable biome dataset at the ORNL Distributed Active Archive Center for Biogeochemical Dynamics ([DAAC](https://daac.ornl.gov/NPP/guides/NPP_EMDI.html). Later in the notebook, I will show how neither the NASA or DAAC data is useful and propose an alternate data source for future work.\n",
    "\n",
    "### Get some metrics from the fires dataset\n",
    "The target variable for this analysis exists inside the wildfire dataset. I start by generating a bounding box of latitude and longitude values to filter the other three sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T05:33:04.349841Z",
     "start_time": "2018-12-04T05:32:52.427363Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate the file path and connect using the sqlite3 driver\n",
    "path = os.path.join('.', 'data', 'FPA_FOD_20170508.sqlite')\n",
    "conn  = sqlite3.connect(path)\n",
    "\n",
    "# retrieving the minimum and maximum latitude and longitude pairs.\n",
    "fires = pd.read_sql_query('''\n",
    "    SELECT \n",
    "        min(LATITUDE)  AS min_lat,\n",
    "        max(LATITUDE)  AS max_lat,\n",
    "        min(LONGITUDE) AS min_lon,\n",
    "        max(LONGITUDE) AS max_lon\n",
    "    FROM\n",
    "        Fires\n",
    "''', conn)\n",
    "\n",
    "# increase by one degree-decimal point so that we don't exclude \n",
    "# nearby weather stations\n",
    "min_lat = np.round(fires.min_lat.values[0], 0)-1\n",
    "min_lon = np.round(fires.min_lon.values[0], 0)-1\n",
    "max_lat = np.round(fires.max_lat.values[0], 0)+1\n",
    "max_lon = np.round(fires.max_lon.values[0], 0)+1\n",
    "\n",
    "# print them to the console\n",
    "min_lat, max_lat, min_lon, max_lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load and process GSOD files\n",
    "The data from NOAA comes in separate zip files, one compressed tar file for each year. Then, each day of the year is yet another compressed gzip file. I extract the main file and remove any years not from 1991-2015. In the next cell I unzip the years we need, then each year into the directory 'gsod_extracted'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T05:11:29.181158Z",
     "start_time": "2018-12-04T05:11:28.983804Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create the file path\n",
    "gsod_path = os.path.join('.', 'data', 'gsod')\n",
    "\n",
    "# make sure the path exists\n",
    "if not os.path.exists(gsod_path):\n",
    "    os.mkdir(gsod_path, parents=True)\n",
    "    \n",
    "# get the main zip file\n",
    "all_years = zipfile.ZipFile(os.path.join('data','gsod_all_years.zip'))\n",
    "\n",
    "# look for contents only in the designated year range\n",
    "members   = [\n",
    "    n for n in all_years.namelist() \n",
    "    if any([n.find(str(yi)) > -1 for yi in list(range(1991, 2016))])\n",
    "]\n",
    "\n",
    "# extract\n",
    "for m in tqdm(members):\n",
    "    all_years.extract(m, gsod_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For each year, extract the daily station files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T00:58:39.405169Z",
     "start_time": "2018-11-27T00:58:39.401840Z"
    },
    "code_folding": [],
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# get the yearly list of tar files\n",
    "years = [f for f in os.listdir(gsod_path) if f.find('tar') > -1]\n",
    "\n",
    "# generate the extract path\n",
    "ex_path = os.path.join('.', 'data', 'gsod_extracted')\n",
    "\n",
    "# make sure the path exists\n",
    "if not os.path.exists(ex_path):\n",
    "    os.mkdir(ex_path, parents=True)\n",
    "    \n",
    "# extract the content from each year into the 'extracted' directory\n",
    "pbar = tqdm(total=len(years))\n",
    "for y in years:\n",
    "    pbar.set_description(y)\n",
    "    \n",
    "    # load the tarfile provided by NOAA\n",
    "    tf = tarfile.TarFile(os.path.join(gsod_path, y))\n",
    "    \n",
    "    # create a subdirectory to extract the contents into\n",
    "    subdir = os.path.join(ex_path, y.replace('.tar', ''))\n",
    "    if not os.path.exists(subdir):\n",
    "        os.mkdir(subdir)\n",
    "    \n",
    "    # extract each year\n",
    "    tf.extractall(subdir)    \n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Process each station file line-by-line into DataFrame. This cell only does the raw transformation from a gzip text file into a csv. Each line of each file is a separate row with each field separated by a certain number of character positions. These are listed in the NOAA GSOD docs and were extensively used to process the data. Note, the extractions do not line up perfectly due to the parser being used. Each column was carefully checked to ensure no missing characters. Also of note is that some of the files contain blank lines so I added a filter at the end of each parsing to only input the row if a valid station id is present. We can't perform the latitude, longitude lookup without it making the row unusable even if it did contain the remaining fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:09:06.898570Z",
     "start_time": "2018-11-27T06:33:56.236257Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the list of years to process\n",
    "years = [f for f in os.listdir(gsod_path) if f.find('tar') > -1]\n",
    "\n",
    "# get the list of files for each day\n",
    "ex_path = os.path.join('.', 'data', 'gsod_extracted')\n",
    "\n",
    "# read and extract the contents for each day of year\n",
    "i=0\n",
    "for y in years:\n",
    "    # create the filename to save the final csv output\n",
    "    name = os.path.join(ex_path, y.replace('.tar', '.csv'))\n",
    "    \n",
    "    # get the subdirectory path\n",
    "    subdir = os.path.join(ex_path, y.replace('.tar', ''))\n",
    "    \n",
    "    # read all files we extracted into the directory\n",
    "    files = os.listdir(subdir)\n",
    "    \n",
    "    # store a list of dictionary objects for each row parsed\n",
    "    content = []\n",
    "\n",
    "    for f in tqdm(files, desc=y):\n",
    "        # open the file\n",
    "        with gzip.open(os.path.join(subdir, f), 'r') as fc:\n",
    "            # read the entire contents, split by newline and ignore header\n",
    "            t = str(fc.read()).split('\\\\n')[1:]\n",
    "\n",
    "            # see GSOD_DESC.txt for exact delimmiter locations\n",
    "            def parse(s):\n",
    "                d = dict(\n",
    "                    stn      = s[ 0: 6].strip(),\n",
    "                    wban     = s[ 6:13].strip(),\n",
    "                    year     = s[13:18].strip(),\n",
    "                    moda     = s[18:23].strip(),\n",
    "                    temp     = s[23:30].strip(),\n",
    "                    temp_cnt = s[30:34].strip(),\n",
    "                    dewp     = s[34:41].strip(),\n",
    "                    dewp_cnt = s[41:44].strip(),\n",
    "                    slp      = s[44:52].strip(),\n",
    "                    slp_cnt  = s[52:55].strip(),\n",
    "                    stp      = s[55:63].strip(),\n",
    "                    stp_cnt  = s[63:66].strip(),\n",
    "                    visib    = s[67:73].strip(),\n",
    "                    visib_cnt= s[73:76].strip(),\n",
    "                    wdsp     = s[76:83].strip(),\n",
    "                    wdsp_cnt = s[83:86].strip(),\n",
    "                    mxspd    = s[88:93].strip(),\n",
    "                    gust     = s[94:101].strip(),\n",
    "                    temp_max = s[102:108].strip(),\n",
    "                    max_temp_flag = s[108:110].strip(),\n",
    "                    temp_min = s[111:116].strip(),\n",
    "                    min_temp_flag = s[116:117].strip(),\n",
    "                    prcp     = s[117:123].strip(),\n",
    "                    prcp_flag= s[123:124].strip(),\n",
    "                    sndp     = s[124:131].strip(),\n",
    "                    frshtt   = s[131:138].strip()\n",
    "                )\n",
    "                \n",
    "                return d if len(d['stn']) > 1 else None\n",
    "\n",
    "            # convert each row into a dictionary using the function above\n",
    "            # and append the contents to the main collection\n",
    "            content += list(map(parse, t))\n",
    "    \n",
    "    # convert the list of dictionaries to a Pandas dataframe\n",
    "    content = pd.DataFrame([c for c in content if c is not None])\n",
    "    \n",
    "    # write this years worth of weather recordings to csv\n",
    "    content.to_csv(name, index=None)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have the data in a usable format, we need join with the weather stations listing in order to gather the latitude and longitude values for each weather summary. I do this by creating a composite key out of USAF and WBAN in both the stations and weather dataframes, then performing an inner join on it. For more information please see the NOAA data documentation provided.\n",
    "\n",
    "I also make sure to exclude weather stations that aren't going to be used in widlfire feature engineering by creating latitude and longitude masks offsetting each min/max by 111km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T05:23:56.754122Z",
     "start_time": "2018-12-04T05:23:56.465537Z"
    },
    "code_folding": [],
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# load the stations file explicitly enforcing datatypes and nan values\n",
    "# also drop any station that doesn't have a latitude or longitude value\n",
    "stations = pd.read_csv(\n",
    "    os.path.join('data', 'isd-history.csv'),\n",
    "    dtype={\n",
    "        'USAF':'str',\n",
    "        'WBAN':'str'\n",
    "    },\n",
    "    na_values={\n",
    "        'WBAN'   :'99999',\n",
    "        'ELEV(M)':'-999'\n",
    "    }\n",
    ").dropna(subset=['LAT', 'LON'], how='any')\n",
    "\n",
    "# take only stations that have lat, lon values within the wildfire range\n",
    "stations['lat_mask'] = [min_lat <= lat <= max_lat for lat in stations.LAT]\n",
    "stations['lon_mask'] = [min_lon <= lon <= max_lon for lon in stations.LON]\n",
    "stations = stations.loc[stations.lat_mask & stations.lon_mask].drop(columns=['lat_mask', 'lon_mask'])\n",
    "\n",
    "# create a key by concatenating the USAF and WBAN cols\n",
    "stations.loc[stations.USAF.isnull(), 'USAF'] = 'none'\n",
    "stations.loc[stations.WBAN.isnull(), 'WBAN'] = 'none'\n",
    "stations['KEY'] = stations.USAF+stations.WBAN\n",
    "\n",
    "# verify key uniqueness\n",
    "assert len(stations.KEY.unique()) == len(stations)\n",
    "\n",
    "# we will only be using these columns\n",
    "stations = stations.reindex(columns=[\n",
    "    'KEY', 'LAT', 'LON', 'ELEV(M)'\n",
    "])\n",
    "\n",
    "# rename the elevation column so we can call it easier\n",
    "stations = stations.rename(columns={'ELEV(M)':'ELEV'})\n",
    "\n",
    "# convert all the column names to lowercase\n",
    "stations.columns = [c.lower() for c in stations.columns]\n",
    "\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the following cell, I go through the csv contents we generated above. Specific datatypes are enforced to prevent Pandas from dropping leading zeroes, for example, and to make additional operations more streamlined. Each will be explained line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T05:30:31.495722Z",
     "start_time": "2018-12-04T05:29:47.704853Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate the file path\n",
    "gsod_path = os.path.join('.', 'data', 'gsod')\n",
    "\n",
    "# get the list of yearly weather files\n",
    "ex_path = os.path.join('.', 'data', 'gsod_extracted')\n",
    "names = [f for f in os.listdir(ex_path) if 'csv' in f]\n",
    "\n",
    "# process each year at a time\n",
    "pbar = tqdm(total=len(names))\n",
    "for name in names:\n",
    "    pbar.set_description(name)\n",
    "    \n",
    "    # load the data, setting data types explicitly or pandas will drop \n",
    "    # the leading zeroes needed for station names. Also, include the \n",
    "    # explicit na values designated in the data documentation\n",
    "    # drop columns we aren't going to use\n",
    "    f1 = pd.read_csv(\n",
    "            os.path.join(ex_path, name), \n",
    "            dtype={\n",
    "                'stn' :'str', \n",
    "                'wban':'str',\n",
    "                'moda':'str',\n",
    "                'frshtt':'str',\n",
    "                'year':'str'},\n",
    "            na_values={\n",
    "                'stn'  :'999999',\n",
    "                'wban' :'99999',\n",
    "                'temp' :'9999.9',\n",
    "                'dewp' :'9999.9',\n",
    "                'slp'  :'9999.9',\n",
    "                'stp'  :'9999.9',\n",
    "                'visib':'999.9',\n",
    "                'wdsp' :'999.9',\n",
    "                'mxspd':'999.9',\n",
    "                'gust' :'999.9',\n",
    "                'max_temp':'9999.9',\n",
    "                'min_temp':'9999.9',\n",
    "                'prcp':'99.9',        \n",
    "                'sndp':'999.9'},\n",
    "        ) \\\n",
    "        .drop(columns=[\n",
    "            'max_temp_flag', 'min_temp_flag', \n",
    "            'temp_cnt', 'dewp_cnt', 'slp_cnt', \n",
    "            'stp_cnt', 'visib_cnt', 'wdsp_cnt'])\n",
    "\n",
    "    # convert the two date columns 'year' and 'moda' to a single pydate\n",
    "    f1['date'] = [\n",
    "        dt.datetime(year=int(r.year), month=int(r.moda[:2]), day=int(r.moda[2:])) \n",
    "        for r in f1.itertuples()\n",
    "    ]\n",
    "    \n",
    "    # extract month number and julian date\n",
    "    f1['month'] = f1.date.apply(lambda x: x.month)\n",
    "    f1['doy'] = f1.date.apply(lambda x: x.timetuple().tm_yday)\n",
    "\n",
    "    # convert prcp values to na where prcp flag is in {'H', 'I'}. see the data docs\n",
    "    f1.loc[(f1.prcp_flag == 'H') | (f1.prcp_flag == 'I'), 'prcp'] = np.nan\n",
    "\n",
    "    # convert 'frshtt' to an ordinal value based on severity where the\n",
    "    # returned value is the number of leading most 1. ie. 010000 -> 2\n",
    "    # 1:fog, 2:rain, 3:snow, 4:hail, 5:thunderstorm, 6:tornado\n",
    "    def fx(x):\n",
    "        x = x[::-1].find('1')\n",
    "        return x if x != -1 else 0\n",
    "\n",
    "    f1['atmos_sev'] = f1.frshtt.apply(fx)\n",
    "    \n",
    "    # create the join key in the same way as we did for weather stations\n",
    "    f1.loc[f1.stn.isnull(), 'stn'] = 'none'\n",
    "    f1.loc[f1.wban.isnull(), 'wban'] = 'none'\n",
    "    f1['key'] = f1.stn + f1.wban\n",
    "    \n",
    "    # perform an inner join with stations\n",
    "    f1 = f1.merge(stations, on='key', how='inner')\n",
    "    \n",
    "    # reorder the columns, dropping the ones that won't be used\n",
    "    prefix = ['lat', 'lon', 'year', 'month', 'doy']\n",
    "    f1 = f1.reindex(columns=prefix + sorted(list(\n",
    "        set(f1.columns) - set(prefix) - {\n",
    "            'moda', 'prcp_flag', 'frshtt', 'stn', 'wban', 'key', 'date'\n",
    "        }\n",
    "    )))\n",
    "    \n",
    "    # write the cleaned dataframe to disk\n",
    "    name = os.path.join(gsod_path, name.replace('.csv', '_cleaned') + '.csv')\n",
    "    f1.to_csv(name, index=None)\n",
    "    \n",
    "    pbar.update(1)\n",
    "    f1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the fires dataset\n",
    "This dataset comes relatively clean. The only modifications we'll be doing is removing the columns we won't be using, creating a few new, and reordering them for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T16:52:03.939805Z",
     "start_time": "2018-12-05T16:50:22.479326Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 892007/892007 [00:16<00:00, 54006.81it/s]\n",
      "100%|██████████| 892007/892007 [00:16<00:00, 53235.42it/s]\n",
      "100%|██████████| 892007/892007 [00:01<00:00, 447479.63it/s]\n",
      "100%|██████████| 892007/892007 [00:03<00:00, 246227.84it/s]\n",
      "100%|██████████| 892007/892007 [00:03<00:00, 224872.39it/s]\n",
      "100%|██████████| 892007/892007 [00:03<00:00, 233895.71it/s]\n",
      "100%|██████████| 892007/892007 [00:16<00:00, 55042.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate the path and connect to the sqlite fires file\n",
    "path = os.path.join('.', 'data', 'FPA_FOD_20170508.sqlite')\n",
    "conn  = sqlite3.connect(path)\n",
    "\n",
    "# read all the columns we need\n",
    "fires = pd.read_sql_query('''\n",
    "    SELECT FOD_ID,\n",
    "        FIRE_YEAR, DISCOVERY_DOY, DISCOVERY_TIME,\n",
    "        STAT_CAUSE_CODE, CONT_DOY, CONT_TIME,\n",
    "        FIRE_SIZE, LATITUDE, LONGITUDE, OWNER_CODE\n",
    "    FROM\n",
    "        Fires;\n",
    "''', conn)\n",
    "\n",
    "# convert column names to lowercase\n",
    "fires.columns = [c.lower() for c in fires.columns]\n",
    "\n",
    "# based on the first 10000 rows, 0.35% have missing containment values which is a \n",
    "# negligible loss at this point in the analysis\n",
    "fires = fires.dropna(subset=[\n",
    "    'discovery_doy', 'discovery_time', 'cont_doy', 'cont_time'\n",
    "], how='any')\n",
    "\n",
    "# convert fire_year, discovery doy, and time to pydate\n",
    "fires['dt_disc'] = [\n",
    "    dt.datetime(year=int(r.fire_year), \n",
    "                month=1, \n",
    "                day=1, \n",
    "                hour=int(r.discovery_time[:2]),\n",
    "                minute=int(r.discovery_time[2:])\n",
    "               ) + \\\n",
    "    dt.timedelta(days=r.discovery_doy)\n",
    "    for r in fires.itertuples()\n",
    "]\n",
    "\n",
    "# convert the containment dates\n",
    "fires['dt_cont'] = [\n",
    "    dt.datetime(year=int(r.fire_year), month=1, day=1, hour=int(r.cont_time[:2]), minute=int(r.cont_time[2:])) + \\\n",
    "    dt.timedelta(days=r.cont_doy)\n",
    "    for r in fires.itertuples()\n",
    "]\n",
    "\n",
    "# create some higher resolution columns\n",
    "def seconds_into_year(x):\n",
    "    a = dt.datetime(year=x.year, month=1, day=1, hour=0, minute=0, second=0)\n",
    "    return int((x-a).total_seconds())\n",
    "\n",
    "def seconds_into_day(x):\n",
    "    a = dt.datetime(year=x.year, month=x.month, day=x.day, hour=0, minute=0, second=0)\n",
    "    return (x-a).seconds\n",
    "\n",
    "# calculate fire duration in seconds, but only if the contained date is\n",
    "# later than the start date\n",
    "fires['disc_soy'] = fires.dt_disc.progress_apply(seconds_into_year)\n",
    "fires['cont_soy'] = fires.dt_cont.progress_apply(seconds_into_year)\n",
    "fires['duration'] = [\n",
    "    r.cont_soy-r.disc_soy \n",
    "    if r.cont_soy > r.disc_soy else np.nan\n",
    "    for r in tqdm(fires.itertuples(), total=len(fires))\n",
    "]\n",
    "\n",
    "# extract month and hour as new columns\n",
    "fires['date']  = fires.dt_disc.progress_apply(lambda x: x.date())\n",
    "fires['month'] = fires.dt_disc.progress_apply(lambda x: x.month)\n",
    "fires['hod']   = fires.dt_disc.progress_apply(lambda x: x.hour)\n",
    "fires['sod']   = fires.dt_disc.progress_apply(seconds_into_day)\n",
    "\n",
    "# drop some columns we won't be using\n",
    "fires = fires.drop(columns=[\n",
    "    'discovery_time', 'cont_doy', 'cont_time', \n",
    "    'disc_soy', 'cont_soy', 'dt_cont', \n",
    "    'dt_disc'\n",
    "])\n",
    "\n",
    "# rename some columns\n",
    "fires = fires.rename(columns={\n",
    "    'discovery_doy':'doy',\n",
    "    'latitude':'lat',\n",
    "    'longitude':'lon',\n",
    "    'fire_year':'year',\n",
    "    'stat_cause_code':'cause_code',\n",
    "})\n",
    "\n",
    "# reorder the columns\n",
    "prefix = ['fod_id', 'lat', 'lon', 'date', 'year', 'month', 'doy', 'hod', 'sod']\n",
    "fires = fires.reindex(columns=prefix + sorted(list(\n",
    "    set(fires.columns) - set(prefix)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a quick look at the categorical variable - OWNER_CODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T17:09:01.138745Z",
     "start_time": "2018-12-05T17:08:47.460697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://plot.ly/~waninger/19\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f58bbd9ec50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the path and connect to the sqlite fires file\n",
    "path = os.path.join('.', 'data', 'FPA_FOD_20170508.sqlite')\n",
    "conn  = sqlite3.connect(path)\n",
    "\n",
    "# get the mapping of cause codes to description\n",
    "owners = pd.read_sql_query('''\n",
    "    SELECT DISTINCT(OWNER_CODE), OWNER_DESCR\n",
    "    FROM Fires;\n",
    "''', conn)\\\n",
    "    .sort_values('OWNER_CODE')\n",
    "\n",
    "# rename the columns and set the index to code\n",
    "owners = owners.rename(columns={\n",
    "    'OWNER_CODE':'code',\n",
    "    'OWNER_DESCR':'owner'\n",
    "}).set_index('code')\n",
    "\n",
    "# get the counts of each cause\n",
    "bincounts = fires.owner_code.value_counts()\n",
    "\n",
    "# plot as a bar plot\n",
    "url = py.plot(go.Figure(\n",
    "    [go.Bar(\n",
    "        x=[owners.loc[idx].owner for idx in bincounts.index],\n",
    "        y=bincounts,\n",
    "        text=bincounts.index,\n",
    "        textposition='outside'\n",
    "    )],\n",
    "    go.Layout(\n",
    "        title='Distribution of owners',\n",
    "        yaxis=dict(title='Count of owned fires')\n",
    "    )\n",
    "), filename='wildfires_original_owner_dist')\n",
    "\n",
    "IFrame(url, width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't our target variable but there are clear commonalities we can take advantage of to boost any signal that may come from the responsible source. To help understand this a bit better here is the list of federal acronyms:\n",
    "\n",
    "* USFS - United States Forest Service\n",
    "* BIA - Bureau of Indian Affairs\n",
    "* BLM - Bureau of Land Management\n",
    "* NPS - National Park Service\n",
    "* FWS - Fish and Wildlife Service\n",
    "* BOR - Bureau of Reclamation\n",
    "\n",
    "Here is a list of things I notice from the visualization.\n",
    "1. UNDEFINED FEDERAL has very little values and can be combined with OTHER FEDERAL. \n",
    "2. COUNTY owned land can be joined with MUNICIPAL/LOCAL.\n",
    "3. STATE OR PRIVATE can be separted into the STATE and PRIVATE categories. To do this, I'll draw from a random binomial distribution characterized by the ratio between the two.\n",
    "4. TRIBAL can be combined with BIA and I'll rename it to Native American.\n",
    "5. BOR will have almost no signal on its own so I'll combine it with OTHER FEDERAL.\n",
    "6. I'll move the FOREIGN items into MISSING/NOT SPECIFIED\n",
    "\n",
    "I also plan on renaming a few before continuing. Additionally, we'll need to include the new owner descriptions in the cleaned fires dataset so we preserve the recategorization mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.to_csv(os.path.join('.', 'data', 'fires_cleaned.csv'), index=None)\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single data frame with cleaned values for all years. This generates a dataframe approximately 1.7gb uncompressed which is a significant reduction from the 3.4gb original compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T01:09:33.030682Z",
     "start_time": "2018-11-28T01:04:15.643829Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get the list of cleaned files\n",
    "files = [f for f in os.listdir(gsod_path) if 'cleaned.csv' in f]\n",
    "assert len(files) == 25\n",
    "\n",
    "gsod = pd.concat([pd.read_csv(os.path.join(gsod_path, f)) for f in files])\n",
    "gsod.to_csv(os.path.join('.', 'data', 'gsod.csv'), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Process ORNL features\n",
    "Each station has a center point and provides the coverage data in both 1km and 50km pixel grids surrounding the station. My first approach to joining the fires and ground cover data was to include any predictions within the station's bounding box but, this led to incredibly sparse results. I leave the cell blocks here to both show my process and why I'm no longer using the data source. In the following cell I load both high and low quality datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:37:48.555808Z",
     "start_time": "2018-11-29T21:37:48.522074Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load the data we'll use, enforce datatypes, and rename columns\n",
    "cover = pd.concat([\n",
    "    pd.read_csv(\n",
    "        os.path.join('.', 'data', f),\n",
    "        usecols=[\n",
    "            'LAT_DD', 'LONG_DD', 'COVR1KM', 'COVR50KM'\n",
    "        ],\n",
    "        dtype={\n",
    "            'COVR1KM':'str',\n",
    "            'COVR50KM':'str'\n",
    "        }\n",
    "    ).rename(columns={\n",
    "        'LAT_DD':'LAT',\n",
    "        'LONG_DD':'LON'\n",
    "    })\n",
    "    for f in [\n",
    "        'EMDI_ClassA_Cover_UMD_81.csv',\n",
    "        'EMDI_ClassB_Cover_UMD_933.csv'\n",
    "    ]\n",
    "], sort=False)\n",
    "    \n",
    "# convert columns to lowercase\n",
    "cover.columns = [c.lower() for c in cover.columns]\n",
    "\n",
    "# create cover 50k grid boundaries\n",
    "cover['lower50_lat'] = cover.lat.apply(lambda x: x-.5)\n",
    "cover['upper50_lat'] = cover.lat.apply(lambda x: x+.5)\n",
    "cover['lower50_lon'] = cover.lon.apply(lambda x: x-.5)\n",
    "cover['upper50_lon'] = cover.lon.apply(lambda x: x+.5)\n",
    "\n",
    "# only include the values within the fire bounding box\n",
    "cover = cover.loc[\n",
    "    (cover.lower50_lat >= min_lat) & (cover.upper50_lat <= max_lat) &\n",
    "    (cover.lower50_lon >= min_lon) & (cover.upper50_lon <= max_lon)\n",
    "]\n",
    "\n",
    "cover.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot a sample of fires and the bounding boxes for each station to show just how inadequate the ORNL dataset is. Each point represents a fire with the size of the fire mapped to the size of the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:55:53.709149Z",
     "start_time": "2018-11-29T21:55:53.526804Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract a uniform sample of 2k fires\n",
    "sample = fires.sample(2000)\n",
    "\n",
    "# generate scatter plot points\n",
    "fire_trace = go.Scatter(\n",
    "    x=sample.lon, \n",
    "    y=sample.lat,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=np.abs(np.log(sample.fire_size))*3,\n",
    "        color='#571C00'\n",
    "    )\n",
    ")\n",
    "\n",
    "# generate the bounding boxes\n",
    "shapes = [\n",
    "    {\n",
    "        'type':'rect',\n",
    "        'x0':r.lower50_lon,\n",
    "        'x1':r.upper50_lon,\n",
    "        'y0':r.lower50_lat,\n",
    "        'y1':r.upper50_lat,\n",
    "        'fillcolor':'rgba(22, 74, 40, .4)',\n",
    "        'line':{\n",
    "            'width':.1\n",
    "        }\n",
    "    }\n",
    "    for r in cover.itertuples()\n",
    "]\n",
    "\n",
    "# plot\n",
    "iplot(go.Figure(\n",
    "    [fire_trace],\n",
    "    layout=go.Layout(\n",
    "        shapes=aRectangles+bRectangles,\n",
    "        xaxis=dict(\n",
    "            title='longitude',\n",
    "            range=[-125, -78]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='latitude',\n",
    "            range=[25, 58]\n",
    "        ),\n",
    "        title='Ground cover data coverage is insufficient',\n",
    "        width=1200,\n",
    "        height=800\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The same goes for soil content because the same stations are used for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:34:56.663890Z",
     "start_time": "2018-11-29T21:34:56.623335Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "soil = pd.concat([\n",
    "    pd.read_csv(\n",
    "        os.path.join('.', 'data', f)\n",
    "    ).rename(columns={\n",
    "        'LAT_DD':'LAT',\n",
    "        'LONG_DD':'LON'\n",
    "    }).drop(columns='SITE_ID')\n",
    "    for f in [\n",
    "        'EMDI_ClassA_Soil_IGBP_81.csv',\n",
    "        'EMDI_ClassB_Soil_IGBP_933.csv'\n",
    "    ]\n",
    "], sort=False)\n",
    "\n",
    "# convert columns to lowercase\n",
    "soil.columns = [c.lower() for c in soil.columns]\n",
    "\n",
    "# create the station bounding box\n",
    "soil['lower50_lat'] = soil.lat.apply(lambda x: x-.5)\n",
    "soil['upper50_lat'] = soil.lat.apply(lambda x: x+.5)\n",
    "soil['lower50_lon'] = soil.lon.apply(lambda x: x-.5)\n",
    "soil['upper50_lon'] = soil.lon.apply(lambda x: x+.5)\n",
    "\n",
    "# only include the values within the fire bounding box\n",
    "soil = soil.loc[\n",
    "    (soil.lower50_lat >= min_lat) & (soil.upper50_lat <= max_lat) &\n",
    "    (soil.lower50_lon >= min_lon) & (soil.upper50_lon <= max_lon)\n",
    "]\n",
    "\n",
    "soil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:57:56.135419Z",
     "start_time": "2018-11-29T21:57:55.955170Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# extract a fire sample\n",
    "sample = fires.sample(2000)\n",
    "\n",
    "# generate the fire scatter points\n",
    "fire_trace = go.Scatter(\n",
    "    x=sample.lon, \n",
    "    y=sample.lat,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=np.abs(np.log(sample.fire_size))*3,\n",
    "        color='#571C00'\n",
    "    )\n",
    ")\n",
    "\n",
    "shapes = [\n",
    "    {\n",
    "        'type':'rect',\n",
    "        'x0':r.lower50_lon,\n",
    "        'x1':r.upper50_lon,\n",
    "        'y0':r.lower50_lat,\n",
    "        'y1':r.upper50_lat,\n",
    "        'fillcolor':'rgba(22, 74, 40, .4)',\n",
    "        'line':{\n",
    "            'width':.1\n",
    "        }\n",
    "    }\n",
    "    for r in soil.itertuples()\n",
    "]\n",
    "\n",
    "# plot\n",
    "iplot(go.Figure(\n",
    "    [fire_trace],\n",
    "    layout=go.Layout(\n",
    "        shapes=aRectangles+bRectangles,\n",
    "        xaxis=dict(\n",
    "            title='longitude',\n",
    "            range=[-125, -78]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='latitude',\n",
    "            range=[25, 58]\n",
    "        ),\n",
    "        title='Soil data coverage is insufficient',\n",
    "        width=1200,\n",
    "        height=800\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An alternative data source for land coverage is available for public use. See the [Earth Engine Data Catalog](https://developers.google.com/earth-engine/datasets/catalog/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Generate aggregate weather features associated with each fire\n",
    "We'll need to lookup all reports within a given bounding box centered at the fire's originating location. I use a bounding box to preclude performing pairwise distance lookups which might be more accurate but will incur a significant expense - $O(n^2)$. The embedded hierarchical structure within a degree-decimal formatted coordinate allows us to generate contextually important containment boundaries. The boundaries will include aggregated values from all weather reports $\\pm$ 55.5km of the fire.\n",
    "\n",
    "This is the long running computation may take several days to complete. I wrote it to perform aggregations in batches. Each batch will cache the resulting features to a csv file and continue with the next. Also of note here is that I use a thread pool rather than a process pool to keep memory usage as low as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T16:38:32.797444Z",
     "start_time": "2018-11-28T16:37:41.605497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load cleaned GSOD file\n",
    "gsod = pd.read_csv(os.path.join('.', 'data', 'gsod.csv'))\n",
    "gsod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T19:27:44.202666Z",
     "start_time": "2018-11-28T16:38:33.176667Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start a thread pool and progress bar\n",
    "pool = TPool(mul.cpu_count())\n",
    "pbar = tqdm(total=len(fires))\n",
    "\n",
    "\n",
    "def weather_agg(args):    \n",
    "    try:\n",
    "        # extract the tuple arguments\n",
    "        fod_id, lat, lon, year, doy = args\n",
    "        \n",
    "        # make a copy of the empty record to start this record with\n",
    "        results = empty.copy()\n",
    "        results['fod_id'] = fod_id\n",
    "        \n",
    "        # get all weather reports within 111km\n",
    "        lat_min, lat_max = lat-.5, lat+.5\n",
    "        lon_min, lon_max = lon-.5, lon+.5\n",
    "        \n",
    "        # retrieve all weather reports within the box and 4 days leading up to and including\n",
    "        # the day of the fire\n",
    "        wthr = gsod.loc[\n",
    "            (gsod.lat >= lat_min) & (gsod.lat <= lat_max) &\n",
    "            (gsod.lon >= lon_min) & (gsod.lon <= lon_max) &\n",
    "            (\n",
    "                (gsod.year == year) & (gsod.doy >= doy-4) & (gsod.doy <= doy) |\n",
    "                (gsod.doy <= 4) & (gsod.year == year-1) & (gsod.doy >= 361+doy)\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # get the three day prior aggregates\n",
    "        w_ = wthr.loc[wthr.doy != doy]\n",
    "        if len(w_) > 0:\n",
    "            results['threeDay_atmos_sev'] = np.mean(w_.atmos_sev)\n",
    "            results['threeDay_temp_max']  = np.max(w_.temp_max)\n",
    "            results['threeDay_temp_min']  = np.min(w_.temp_min)\n",
    "            results['threeDay_temp']  = np.median(w_.temp)\n",
    "            results['threeDay_sndp']  = np.median(w_.sndp)\n",
    "            results['threeDay_dewp']  = np.median(w_.dewp)\n",
    "            results['threeDay_gust']  = np.max(w_.gust)\n",
    "            results['threeDay_mxspd'] = np.max(w_.mxspd)\n",
    "            results['threeDay_stp']   = np.median(w_.stp)\n",
    "            results['threeDay_temp']  = np.median(w_.temp)\n",
    "            results['threeDay_slp']   = np.median(w_.slp)\n",
    "            results['threeDay_wdsp']  = np.median(w_.wdsp)\n",
    "            results['threeDay_prcp']  = np.sum(w_.prcp)\n",
    "            results['threeDay_visib'] = np.median(w_.visib)   \n",
    "\n",
    "        # get the dayOf aggregates   \n",
    "        w_ = wthr.loc[wthr.doy == doy]\n",
    "        if len(w_) > 0:            \n",
    "            results['dayOf_atmos_sev'] = np.mean(w_.atmos_sev)\n",
    "            results['dayOf_temp_max']  = np.max(w_.temp_max)\n",
    "            results['dayOf_temp_min']  = np.min(w_.temp_min)\n",
    "            results['dayOf_temp']  = np.median(w_.temp)\n",
    "            results['dayOf_sndp']  = np.median(w_.sndp)\n",
    "            results['dayOf_dewp']  = np.median(w_.dewp)\n",
    "            results['dayOf_gust']  = np.max(w_.gust)\n",
    "            results['dayOf_mxspd'] = np.max(w_.mxspd)\n",
    "            results['dayOf_stp']   = np.median(w_.stp)\n",
    "            results['dayOf_temp']  = np.median(w_.temp)\n",
    "            results['dayOf_slp']   = np.median(w_.slp)\n",
    "            results['dayOf_wdsp']  = np.median(w_.wdsp)\n",
    "            results['dayOf_prcp']  = np.median(w_.prcp)\n",
    "            results['dayOf_visib'] = np.median(w_.visib)   \n",
    "    \n",
    "    # catch all exceptions and continue gracefully but make sure we\n",
    "    # notify in case any occur\n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        print(exc_type, e, exc_tb.tb_lineno)\n",
    "    \n",
    "    pbar.update(1)\n",
    "    return results\n",
    "\n",
    "# create the dayOf columns\n",
    "excols = {'lat', 'lon', 'elev', 'year', 'month', 'doy', 'fod_id'}\n",
    "daily_cols    = ['dayOf_'    + c for c in list(set(gsod.columns) - excols)]\n",
    "threeDay_cols = ['threeDay_' + c for c in list(set(gsod.columns) - excols)]\n",
    "\n",
    "# create an empty dictionary to start each feature row\n",
    "empty = dict()\n",
    "for c in daily_cols+threeDay_cols:\n",
    "    empty[c] = np.nan\n",
    "\n",
    "# perform this operation in batches caching the fire results each iteration\n",
    "start, step = 0, 10000\n",
    "for i in range(0, len(fires), step):\n",
    "    # get the set of indices to process\n",
    "    idx_set = fires.index.tolist()[i:i+step]\n",
    "    \n",
    "    # process\n",
    "    batch = pool.map(weather_agg, [\n",
    "        (r.fod_id, r.lat, r.lon, r.year, r.doy) \n",
    "        for r in fires.loc[idx_set].itertuples()\n",
    "    ])\n",
    "    \n",
    "    # cache\n",
    "    pd.DataFrame(batch).to_csv(os.path.join('.', 'data', 'fires', f'fires_b{i}.csv'), index=None)\n",
    "    \n",
    "pool.close(); pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, read all batches into a single dataframe and write it back to disk as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T20:12:05.097872Z",
     "start_time": "2018-11-29T20:11:45.784546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# combine the batches into a single dataframe\n",
    "path = os.path.join('.', 'data', 'fires')\n",
    "fire_weather = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(os.path.join(path, f)) \n",
    "        for f in os.listdir(path) if '.csv' in f\n",
    "    ],\n",
    "    sort=False\n",
    ")\n",
    "\n",
    "# write the combined dataframe to disk\n",
    "path = os.path.join('.', 'data', 'fire_weather.csv')\n",
    "fire_weather.to_csv(path, index=None)\n",
    "\n",
    "fire_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create the combined file to use for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:14:35.301610Z",
     "start_time": "2018-12-05T15:14:28.571023Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load the cleaned fires data\n",
    "path = os.path.join('.', 'data', 'fires_cleaned.csv')\n",
    "fires = pd.read_csv(path, parse_dates=['date'])\n",
    "\n",
    "# load the weather aggregations\n",
    "path = os.path.join('.', 'data', 'fire_weather.csv')\n",
    "weather = pd.read_csv(path)\n",
    "\n",
    "# merge the dataframes on the fod_id\n",
    "df = fires.merge(weather, on='fod_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:14:43.375890Z",
     "start_time": "2018-12-05T15:14:40.748376Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def nan_percentages(df, show_zero=False):\n",
    "    cols = sorted(df.columns)\n",
    "    d, p = len(df), {}\n",
    "\n",
    "    for col in cols:\n",
    "        a = sum(pd.isnull(df[col]))\n",
    "        p[col] = a/d\n",
    "\n",
    "    for k, v in p.items():\n",
    "        n = len(k) if len(k) <= 20 else 20\n",
    "        v = np.round(v, 4)\n",
    "        \n",
    "        if v != 0 or show_zero:\n",
    "            print('{:<20} {:<5}'.format(k[:n], v))\n",
    "\n",
    "compute_cols = list(set(df.columns) - {'fod_id', 'date'})        \n",
    "nan_percentages(df[compute_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First off, we notice that nearly 13% of our rows weren't recorded correctly. Those are the records where the contanment date was recorded before the discovery date. Let's drop those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:15:02.679935Z",
     "start_time": "2018-12-05T15:15:02.082517Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[[not b for b in df.duration.isnull()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T21:26:26.042079Z",
     "start_time": "2018-12-04T21:26:25.800951Z"
    },
    "hidden": true
   },
   "source": [
    "We have quite a few NA values in the resulting weather data and I'm running out of time to do any complex fixes. For the purposes of this project we're going to make some quick assumptions and transformations. Lets see how much of the dataset doesn't have any dayOf features at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:15:11.525025Z",
     "start_time": "2018-12-05T15:15:11.375339Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.round(len(df.loc[\n",
    "    df.dayOf_prcp.isnull() &\n",
    "    df.dayOf_visib.isnull() &\n",
    "    df.dayOf_gust.isnull() &\n",
    "    df.dayOf_dewp.isnull() & \n",
    "    df.dayOf_temp_max.isnull() &\n",
    "    df.dayOf_temp_min.isnull() &\n",
    "    df.dayOf_temp.isnull() &\n",
    "    df.dayOf_atmos_sev.isnull() &\n",
    "    df.dayOf_wdsp.isnull() &\n",
    "    df.dayOf_mxspd.isnull()\n",
    "])/len(df)*100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That's quite a high percentage and accounts for many of the missing values. Lets drop those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:15:16.222217Z",
     "start_time": "2018-12-05T15:15:15.624729Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\n",
    "    'dayOf_prcp', 'dayOf_visib', 'dayOf_gust', 'dayOf_dewp',\n",
    "    'dayOf_temp_max', 'dayOf_temp_min', 'dayOf_temp',\n",
    "    'dayOf_atmos_sev', 'dayOf_wdsp', 'dayOf_mxspd'\n",
    "], how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next lets look at sndp - snow depth. This column is almost completely nan but we don't have to lose the information. Lets transform this column into an indicator that simply says whether or not snow was present at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:15:21.336147Z",
     "start_time": "2018-12-05T15:15:20.592051Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create the indicators\n",
    "df['threeDay_snow'] = [1 if not b else 0 for b in df.threeDay_sndp.isnull()]\n",
    "df['dayOf_snow']    = [1 if not b else 0 for b in df.dayOf_sndp.isnull()]\n",
    "\n",
    "# drop the original\n",
    "df = df.drop(columns=['threeDay_sndp', 'dayOf_sndp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next highest source of missing values is in our pressure columns: slp and stp. I'm going to drop these columns all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:15:25.003510Z",
     "start_time": "2018-12-05T15:15:24.666174Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop the pressure columns\n",
    "df = df.drop(columns=[\n",
    "    'dayOf_stp', 'dayOf_slp', 'threeDay_stp', 'threeDay_slp'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now lets take the missing gust values. For this, lets just take the maximum recorded windspeed for the day and three day respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:15:28.380713Z",
     "start_time": "2018-12-05T15:15:28.358985Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.loc[df.dayOf_gust.isnull(), 'dayOf_gust'] = df.loc[df.dayOf_gust.isnull(), 'dayOf_mxspd']\n",
    "df.loc[df.threeDay_gust.isnull(), 'threeDay_gust'] = df.loc[df.threeDay_gust.isnull(), 'threeDay_mxspd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I use linear regression models to impute any of the remaining missing values. In the next cell, I loop through each collumn with missing values generating a model for each. I use these individual models to predict the remaining missing values. This preserves any existing relationship that may exist between the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:16:01.789111Z",
     "start_time": "2018-12-05T15:15:31.711813Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get the remaining columns with nan values\n",
    "to_impute = [c for c in df.columns if sum(df.loc[:, c].isnull()) > 0]\n",
    "\n",
    "# make sure we don't use these columns in the regression model\n",
    "excluded_columns = {\n",
    "    'fod_id', 'date', 'year', 'sod', 'cause_code', \n",
    "    'duration', 'fire_size', 'owner_code', 'dayOf_snow',\n",
    "    'threeDay_snow'\n",
    "}\n",
    "\n",
    "# impute each column\n",
    "for c in tqdm(to_impute):\n",
    "    # extract the rows that need imputed\n",
    "    x = df[[not b for b in df.loc[:, c].isnull()]]\n",
    "    \n",
    "    # get the column names to use \n",
    "    inputs = set(df.columns) - excluded_columns - {c}\n",
    "    \n",
    "    # create the r-style formula\n",
    "    formula = c + '~' + '+'.join(inputs)\n",
    "    \n",
    "    # build and fit the model\n",
    "    model = smf.ols(formula=formula, data=df).fit()\n",
    "    \n",
    "    # make predictions\n",
    "    predictions = model.predict(exog=df.loc[\n",
    "        df.loc[:, c].isnull(), inputs\n",
    "    ])\n",
    "    \n",
    "    # ensure predictions aren't negative\n",
    "    predictions = [p if p > 0 else 0 for p in predictions]\n",
    "    \n",
    "    # set the missing vals to the predicted\n",
    "    df.loc[df.loc[:, c].isnull(), c] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As a final check lets print the percentage of nan values to make sure we've generated a complete dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:17:42.698637Z",
     "start_time": "2018-12-05T15:17:41.982733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "compute_cols = list(set(df.columns) - {'fod_id', 'date'})    \n",
    "nan_percentages(df[compute_cols], show_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:18:11.616559Z",
     "start_time": "2018-12-05T15:17:54.559885Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# write it to disk\n",
    "path = os.path.join('.', 'data', 'fires_complete.csv')\n",
    "df.to_csv(path, index=None)\n",
    "\n",
    "# show it\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T06:43:48.217289Z",
     "start_time": "2018-12-04T06:43:48.183312Z"
    }
   },
   "source": [
    "### Q1. What are the most important indicators when determining the strength of a wildfire?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets get an idea of the distribution of classes in the dataset. We'll do this by querying the distinct cause descriptions for the original fires dataset and visualizing the counts of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:19:10.229586Z",
     "start_time": "2018-12-05T15:19:07.886220Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the cleaned fire data\n",
    "path = os.path.join('.', 'data', 'fires_complete.csv')\n",
    "df = pd.read_csv(path, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T21:07:21.955510Z",
     "start_time": "2018-12-04T21:07:21.948499Z"
    }
   },
   "source": [
    "In Part 1 we left the dataframe in a more dense form by not expanding the categorical variables. We only have one of those features: owner code. Lets go ahead and convert them before proceeding any further. First, I query the orginal fires dataset to get the mapping of owner code to description to give us a bit more context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:32:46.500304Z",
     "start_time": "2018-12-05T15:32:46.495641Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:19:47.181121Z",
     "start_time": "2018-12-05T15:19:35.208821Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate the path and connect to the sqlite fires file\n",
    "path = os.path.join('.', 'data', 'FPA_FOD_20170508.sqlite')\n",
    "conn  = sqlite3.connect(path)\n",
    "\n",
    "# get the mapping of cause codes to description\n",
    "cause_map = pd.read_sql_query('''\n",
    "    SELECT DISTINCT(STAT_CAUSE_CODE), STAT_CAUSE_DESCR\n",
    "    FROM Fires;\n",
    "''', conn)\\\n",
    "    .sort_values('STAT_CAUSE_CODE')\n",
    "\n",
    "# rename the columns and set the index to code\n",
    "cause_map = cause_map.rename(columns={\n",
    "    'STAT_CAUSE_CODE':'code',\n",
    "    'STAT_CAUSE_DESCR':'cause'\n",
    "}).set_index('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:19:59.325388Z",
     "start_time": "2018-12-05T15:19:53.341793Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get the counts of each cause\n",
    "bincounts = df.cause_code.value_counts()\n",
    "\n",
    "# plot as a bar plot\n",
    "iplot(go.Figure(\n",
    "    [go.Bar(\n",
    "        x=[cause_map.loc[idx].cause for idx in bincounts.index],\n",
    "        y=bincounts,\n",
    "    )],\n",
    "    go.Layout(\n",
    "        title='Distribution of causes is not uniformly distributed',\n",
    "        yaxis=dict(title='Count of fires')\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this visualization we see difficulties beginning to form. The classes are far from uniformly distributed which makes predicting the lower represented classes more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:27:13.948765Z",
     "start_time": "2018-12-05T15:27:13.944622Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "compute_cols\n",
    "\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T15:26:03.561736Z",
     "start_time": "2018-12-05T15:26:03.557624Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hcds-final]",
   "language": "python",
   "name": "conda-env-hcds-final-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "563.333px",
    "left": "640.333px",
    "right": "20px",
    "top": "211px",
    "width": "621.323px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
