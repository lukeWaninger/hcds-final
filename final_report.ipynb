{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Source of Wildfires\n",
    "Luke Waninger  \n",
    "DATA 512 Final Project  \n",
    "University of Washington, Fall 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wildfires have been a big topic in the recent news with devasting effects across the western coast of the United States. So far this year, we have had less burn than 2017, but the current fire in California is the largest in state history and still burns rapidly. Last year, we had almost 2 billion dollars of losses across the United States as a result of wildfire damage which has been the highest in history [[6](https://www.iii.org/fact-statistic/facts-statistics-wildfires)]. Risks of wildfires continue to climb as scientists discover alarming links between rising greenhouse gasses, temperature, and wildfire severity. N. P. Gillett et al. performed a comprehensive study on the relationship between the two and concluded with overwhelming confidence that a positive trend exists between them [[2](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2004GL020876)]. Rising greenhouse gasses could be playing a significant role in the prevalence and severity of forest fires. The visualization below shows a scatter plot of wildfires recorded in the continental US from 1991-2015. Each point is a fire (I'll go into the colors later in the notebook). This picture shows the the magnitude and prevalance of the problem here at home and gives me further creedance to study the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T21:44:36.737772Z",
     "start_time": "2019-01-11T21:44:36.588556Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(os.path.join('images', 'all_fires_map.JPG'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key to understanding the overall problem is the double-edged sword forests play in climate change; they are both a cause and effect. The wildfires both increase atmospheric greenhouse gasses and destroy the integral vegetation to the planet's carbon cycle [[3](https://www.iucn.org/resources/issues-briefs/forests-and-climate-change), [7](https://daac.ornl.gov/NPP/guides/NPP_EMDI.html), [8](http://daac.ornl.gov/)]. The Paris Agreement has specifically mentioned the importance of this and insists that countries protect against deforestation [[4](https://unfccc.int/process-and-meetings/the-paris-agreement/the-paris-agreement)]. Not only is the world pushing to keep the forests we have but here at home, we have begun to employ them as significant combatants in the fight against climate change. California has led the way with their proposed carbon plan. It proposes methods to reshape parts of their existing ecosystem to make their forests even more efficient at removing carbon [[5](http://www.unenvironment.org/news-and-stories/story/forests-provide-critical-short-term-solution-climate-change)]. Stopping deforestation would significantly promote the UNs progress towards reaching goals outlined in the Paris Agreement.\n",
    "\n",
    "However, this will not work if the forests continue in the same destructive cycle with our ecosystem. The goal of this project is two-fold. One, to understand the independent variables and correlation effects in a combined dataset of the Fire Program Analysis (FPA) reporting system, NOAA's Global Surface Summary of Day Data (GSOD) 7, and  NASA's biomass indicators. Two, to train and assess a model for predicting the reason a wildfire started. (and possibly estimate the impact? location?) Identifying the source is a difficult task for investigators in the wild. The vastness of land covered is much larger than the matchstick or location of a lightning strike. Developing an understanding of the independent variables and a reliable prediction model could give authorities valuable direction as to where to begin their search.\n",
    "\n",
    "#### Research Questions\n",
    "* What are the most important indicators to consider when determining the cause of a wildfire?\n",
    "* Can a reliable model be built to assist investigators in determining the cause of a wildfire?\n",
    "\n",
    "####  Reproducibility\n",
    "This notebook is intended to be completely reproducible. However, the starting datasets are much too large to be hosted on GitHub. I provide a small, randomly selected sample with the repository to show the dataset cleaning and generation process. If you run this notebook on your own machine please be aware that the notebook requires quite a bit of resources. With 12 cores running at 4ghz and a consistent 95% CPU load, it took my machine nearly 27 hours to compute. The analysis portion of the notebook is also computationally expensive. The cross-validation approach implemented will consume all available resources and severely limit any other concurrent processes for several hours. The final tuned models can be computed directly via the parameters found during my tuning process.\n",
    "\n",
    "The original data format of the GSOD data makes creating samples a bit challenging. To do this, I ran an additional notebook with the following code. It opens each subdir of the extracted GSOD file and randomly selects and removes half the files. I ran this iteratively until the resulting file size was within the Github file size limit of 100mb.\n",
    "\n",
    "```Python\n",
    "import os\n",
    "\n",
    "# walk the extracted directory\n",
    "for dirpath, dirnames, filenames in os.walk('gsod_all_years'):\n",
    "    \n",
    "    # process each year\n",
    "    for sdir in dirnames:\n",
    "        # randomly select some station files        \n",
    "        sfiles = os.listdir(os.path.join(dirpath, sdir))\n",
    "        to_remove = np.random.choice(sfiles, int(len(sfiles)/2))\n",
    "        \n",
    "        # remove them\n",
    "        for f in to_remove:\n",
    "            try:\n",
    "                tr = os.path.join('.', dirpath, sdir, f)\n",
    "                os.remove(tr)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "```\n",
    "\n",
    "I repacked the sample to be in the same format as the original dataset using WinZip. To sample from the completed fires dataset I used the following code snippet.\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "\n",
    "# read\n",
    "df = pd.read_csv('fires_complete.csv')\n",
    "\n",
    "# sample\n",
    "df = df.sample(frac=.85)\n",
    "\n",
    "# write back\n",
    "df.to_csv('fires_complete.csv', index=None)\n",
    "```\n",
    "\n",
    "And finally, to sample the fires data I first dropped all other tables besides Fires. Next, I ran the following snippet iteratively until the sqlite file was under 100mb.\n",
    "\n",
    "```Python\n",
    "import sqlite3\n",
    "\n",
    "# connect\n",
    "path = os.path.join('FPA_FOD_20170508.sqlite')\n",
    "conn = sqlite3.connect(path)\n",
    "\n",
    "# randomly delete some fires\n",
    "conn.execute(\"\"\"\n",
    "  DELETE FROM Fires\n",
    "  WHERE fod_id IN (\n",
    "    SELECT fod_id\n",
    "    FROM Fires \n",
    "    ORDER BY RANDOM() \n",
    "    LIMIT 100000\n",
    "  );\n",
    "\"\"\")\n",
    "\n",
    "# compress the file\n",
    "conn.execute('VACUUM;')\n",
    "```\n",
    "\n",
    "#### A Note on Visualizations\n",
    "I use [Plotly](https://plot.ly/python/) extensively throughout this notebook. They are interactive and require Javascript to be running in the background. The Github previewer does not run the necessary Javascript for rendering making them just empty grey squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Full Datasets\n",
    "The full datasets can be downloaded by changing the bool `fulldata` to True and running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T21:44:36.762777Z",
     "start_time": "2019-01-11T21:44:36.744768Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fulldata = False\n",
    "\n",
    "if fulldata:\n",
    "    import requests\n",
    "\n",
    "    bucket = 'https://s3.amazonaws.com/waninger-hcds/'\n",
    "    \n",
    "    fires_original = 'FPA_FOD_20170508.sqlite'\n",
    "    gsod_original  = 'gsod_all_years.zip'\n",
    "    \n",
    "    # download the complete fires dataset\n",
    "    print('fetching fires data')\n",
    "    r = requests.get(bucket+fires_original, allow_redirects=True)\n",
    "    if r.ok:\n",
    "        with open(os.path.join('.', 'data', fires_complete), 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    \n",
    "    # download the original GSOD \n",
    "    print('fetching GSOD data')\n",
    "    r = requests.get(bucket+gsod_original, allow_redirects=True)\n",
    "    if r.ok:\n",
    "        with open(os.path.join('.', 'data', gsod_original), 'wb') as f:\n",
    "            f.write(r.content)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "This notebook is coded to run with Python 3.6. Several libraries from the Python standard library will be used along with several third-party modules. These can be installed with the provided requirements file using the command \n",
    "\n",
    "`pip install --user -r requirements.txt`\n",
    "\n",
    "More information regarding the standard libarary can be found at [python.org](https://docs.python.org/3.6/library/index.html).\n",
    "\n",
    "For third party libraries please see:\n",
    "* [numpy == 1.13.0](https://docs.scipy.org/doc/numpy-1.13.0/reference/)\n",
    "* [pandas == 0.23.4](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "* [plotly == 3.4.2](https://plot.ly/python/)\n",
    "* [scikit-learn == 0.20.1](https://scikit-learn.org/stable/documentation.html)\n",
    "* [statsmodels == 0.9.0](https://www.statsmodels.org/stable/index.html)\n",
    "* [tqdm == 4.28.1](https://github.com/tqdm/tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T21:44:48.049216Z",
     "start_time": "2019-01-11T21:44:36.765944Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Python standard library\n",
    "import calendar\n",
    "import datetime as dt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import itertools as it\n",
    "import multiprocessing as mul\n",
    "from multiprocessing.dummy import Pool as TPool\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "# third party modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import label_binarize, LabelEncoder, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "# initialize plotly    \n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# set notebook options\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# initalize tqdm\n",
    "tqdm.pandas(leave=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "Four data sources are to be used for this project. The primary data source was found through Kaggle and contains 1.88 million wildfires that occurred in the United States from 1992 to 2015. This data contains the primary labels to be used as target variables. The United States Department of Agriculture curated the original data ([Forest Service](https://www.fs.fed.us/)) and can be found at [link](https://www.fs.usda.gov/rds/archive/Product/RDS-2013-0009.4/). The second is the GSOD data curated by [NOAA](https://www.noaa.gov/). Finally, the National Air and Space Association (NASA) hosts a valuable biome dataset at the ORNL Distributed Active Archive Center for Biogeochemical Dynamics ([DAAC](https://daac.ornl.gov/NPP/guides/NPP_EMDI.html). Later in the notebook, I will show how neither the NASA or DAAC data is useful and propose an alternate data source for future work.\n",
    "\n",
    "### Get some metrics from the fires dataset\n",
    "The target variable for this analysis exists inside the wildfire dataset. I start by generating a bounding box of latitude and longitude values to filter the other three sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.608Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# generate the file path and connect using the sqlite3 driver\n",
    "path = os.path.join('.', 'data', 'FPA_FOD_20170508.sqlite')\n",
    "conn  = sqlite3.connect(path)\n",
    "\n",
    "# retrieving the minimum and maximum latitude and longitude pairs.\n",
    "fires = pd.read_sql_query('''\n",
    "    SELECT \n",
    "        min(LATITUDE)  AS min_lat,\n",
    "        max(LATITUDE)  AS max_lat,\n",
    "        min(LONGITUDE) AS min_lon,\n",
    "        max(LONGITUDE) AS max_lon\n",
    "    FROM\n",
    "        Fires\n",
    "''', conn)\n",
    "\n",
    "# increase by one degree-decimal point so that we don't exclude \n",
    "# nearby weather stations\n",
    "min_lat = np.round(fires.min_lat.values[0], 0)-1\n",
    "min_lon = np.round(fires.min_lon.values[0], 0)-1\n",
    "max_lat = np.round(fires.max_lat.values[0], 0)+1\n",
    "max_lon = np.round(fires.max_lon.values[0], 0)+1\n",
    "\n",
    "# print them to the console\n",
    "min_lat, max_lat, min_lon, max_lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process GSOD files\n",
    "The data from NOAA comes in a full-nonsense version. It's a collection zipped zip files, one compressed tar file for each year. Then, each day of the year and station is yet another compressed gzip file. I extract the main file and remove any years not from 1991-2015. In the next cell I unzip the years we need, then each year into the directory 'gsod_extracted'. I apologize for this next few cells. This really is nonsense to make it reproducible all the way from the source file from NOAA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.613Z"
    },
    "code_folding": [
     4,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# create the file path\n",
    "gsod_path = os.path.join('.', 'data', 'gsod')\n",
    "\n",
    "# make sure the path exists\n",
    "if not os.path.exists(gsod_path):\n",
    "    os.mkdir(gsod_path)\n",
    "    \n",
    "# get the main zip file\n",
    "all_years = zipfile.ZipFile(os.path.join('data','gsod_all_years.zip'))\n",
    "\n",
    "# look for contents only in the designated year range\n",
    "members   = [\n",
    "    n for n in all_years.namelist() \n",
    "    if any([n.find(str(yi)) > -1 for yi in list(range(1991, 2016))])\n",
    "]\n",
    "\n",
    "# extract\n",
    "for m in tqdm(members):\n",
    "    t = all_years.extract(m, gsod_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first load the listing of weather stations. I do this first because it reduces the number of operations of depending cells by quite a bit and in turn, drastically speeds up the notebook. Furthermore, I need the latitude and longitude values for each weather summary in order to join with the fires dataset. I do this by creating a composite key out of USAF and WBAN in both the stations and weather dataframes, then performing an inner join on it. For more information please see the NOAA data documentation provided. And finally, I need to create a smaller subset of the original dataset to reduce the amount of data we need to upload/download for reproducibility purposes.\n",
    "\n",
    "I also make sure to exclude weather stations that aren't going to be used in widlfire feature engineering by creating latitude and longitude masks offsetting each min/max by 111km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.617Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# load the stations file explicitly enforcing datatypes and nan values\n",
    "# also drop any station that doesn't have a latitude or longitude value\n",
    "stations = pd.read_csv(\n",
    "    os.path.join('data', 'isd-history.csv'),\n",
    "    dtype={\n",
    "        'USAF':'str',\n",
    "        'WBAN':'str'\n",
    "    },\n",
    "    na_values={\n",
    "        'WBAN'   :'99999',\n",
    "        'ELEV(M)':'-999'\n",
    "    }\n",
    ").dropna(subset=['LAT', 'LON'], how='any')\n",
    "\n",
    "# take only stations that have lat, lon values within the wildfire range\n",
    "stations['lat_mask'] = [min_lat <= lat <= max_lat for lat in stations.LAT]\n",
    "stations['lon_mask'] = [min_lon <= lon <= max_lon for lon in stations.LON]\n",
    "stations = stations.loc[stations.lat_mask & stations.lon_mask].drop(columns=['lat_mask', 'lon_mask'])\n",
    "\n",
    "# create a key by concatenating the USAF and WBAN cols\n",
    "stations.loc[stations.USAF.isnull(), 'USAF'] = 'none'\n",
    "stations.loc[stations.WBAN.isnull(), 'WBAN'] = 'none'\n",
    "stations['KEY'] = stations.USAF+stations.WBAN\n",
    "\n",
    "# verify key uniqueness\n",
    "assert len(stations.KEY.unique()) == len(stations)\n",
    "\n",
    "# we will only be using these columns\n",
    "stations = stations.reindex(columns=[\n",
    "    'KEY', 'LAT', 'LON', 'ELEV(M)'\n",
    "])\n",
    "\n",
    "# rename the elevation column so we can call it easier\n",
    "stations = stations.rename(columns={'ELEV(M)':'ELEV'})\n",
    "\n",
    "# convert all the column names to lowercase\n",
    "stations.columns = [c.lower() for c in stations.columns]\n",
    "\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the contents of each year into the extracted directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.620Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# get the yearly list of tar files\n",
    "years = [f for f in os.listdir(gsod_path) if f.find('tar') > -1]\n",
    "\n",
    "# generate the extract path\n",
    "ex_path = os.path.join('.', 'data', 'gsod_extracted')\n",
    "\n",
    "# make sure the path exists\n",
    "if not os.path.exists(ex_path):\n",
    "    os.mkdir(ex_path)\n",
    "    \n",
    "# extract the content from each year into the 'extracted' directory\n",
    "pbar = tqdm(total=len(years))\n",
    "for y in years:\n",
    "    pbar.set_description(y)\n",
    "    \n",
    "    # load the tarfile provided by NOAA\n",
    "    tf = tarfile.TarFile(os.path.join(gsod_path, y))\n",
    "    \n",
    "    # create a subdirectory to extract the contents into\n",
    "    subdir = os.path.join(ex_path, y.replace('.tar', ''))\n",
    "    if not os.path.exists(subdir):\n",
    "        os.mkdir(subdir)\n",
    "    \n",
    "    # extract each year\n",
    "    tf.extractall(subdir)    \n",
    "    pbar.update(1)\n",
    "    \n",
    "# otherwise this is the sampled data so just move the contents\n",
    "if len(years) == 0:\n",
    "    years = os.listdir(gsod_path)\n",
    "    for y in years:\n",
    "        files = os.listdir(os.path.join(gsod_path, y))\n",
    "        for f in files:\n",
    "            old = os.path.join(gsod_path, y, f)\n",
    "            \n",
    "            newdir = os.path.join(ex_path, y)\n",
    "            if not os.path.exists(newdir):\n",
    "                os.mkdir(newdir)\n",
    "\n",
    "            new = os.path.join(newdir, f)\n",
    "            os.rename(old, new)\n",
    "        pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each station file line-by-line into DataFrame. This cell only does the raw transformation from a gzip text file into a csv. Each line of each file is a separate row with each field separated by a certain number of character positions. These are listed in the NOAA GSOD docs and were extensively used to process the data. Note, the extractions do not line up perfectly due to the parser being used. Each column was carefully checked to ensure no missing characters. Also of note is that some of the files contain blank lines so I added a filter at the end of each parsing to only input the row if a valid station id is present. We can't perform the latitude, longitude lookup without it making the row unusable even if it did contain the remaining fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.624Z"
    },
    "code_folding": [
     28
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the list of files for each day\n",
    "ex_path = os.path.join('.', 'data', 'gsod_extracted')\n",
    "years = [d for d in os.listdir(ex_path) if os.path.isdir(os.path.join(ex_path, d))]\n",
    "\n",
    "# read and extract the contents for each day of year\n",
    "i=0\n",
    "for y in years:\n",
    "    # create the filename to save the final csv output\n",
    "    name = os.path.join(ex_path, y.replace('.tar', ''))\n",
    "    name = name + '.csv'\n",
    "    \n",
    "    # get the subdirectory path\n",
    "    subdir = os.path.join(ex_path, y.replace('.tar', ''))\n",
    "    \n",
    "    # read all files we extracted into the directory\n",
    "    files = os.listdir(subdir)\n",
    "    \n",
    "    # store a list of dictionary objects for each row parsed\n",
    "    content = []\n",
    "\n",
    "    for f in tqdm(files, desc=y):\n",
    "        # open the file\n",
    "        with gzip.open(os.path.join(subdir, f), 'r') as fc:\n",
    "            # read the entire contents, split by newline and ignore header\n",
    "            t = str(fc.read()).split('\\\\n')[1:]\n",
    "\n",
    "            # see GSOD_DESC.txt for exact delimmiter locations\n",
    "            def parse(s):\n",
    "                d = dict(\n",
    "                    stn      = s[ 0: 6].strip(),\n",
    "                    wban     = s[ 6:13].strip(),\n",
    "                    year     = s[13:18].strip(),\n",
    "                    moda     = s[18:23].strip(),\n",
    "                    temp     = s[23:30].strip(),\n",
    "                    temp_cnt = s[30:34].strip(),\n",
    "                    dewp     = s[34:41].strip(),\n",
    "                    dewp_cnt = s[41:44].strip(),\n",
    "                    slp      = s[44:52].strip(),\n",
    "                    slp_cnt  = s[52:55].strip(),\n",
    "                    stp      = s[55:63].strip(),\n",
    "                    stp_cnt  = s[63:66].strip(),\n",
    "                    visib    = s[67:73].strip(),\n",
    "                    visib_cnt= s[73:76].strip(),\n",
    "                    wdsp     = s[76:83].strip(),\n",
    "                    wdsp_cnt = s[83:86].strip(),\n",
    "                    mxspd    = s[88:93].strip(),\n",
    "                    gust     = s[94:101].strip(),\n",
    "                    temp_max = s[102:108].strip(),\n",
    "                    max_temp_flag = s[108:110].strip(),\n",
    "                    temp_min = s[111:116].strip(),\n",
    "                    min_temp_flag = s[116:117].strip(),\n",
    "                    prcp     = s[117:123].strip(),\n",
    "                    prcp_flag= s[123:124].strip(),\n",
    "                    sndp     = s[124:131].strip(),\n",
    "                    frshtt   = s[131:138].strip()\n",
    "                )\n",
    "                \n",
    "                return d if len(d['stn']) > 1 else None\n",
    "\n",
    "            # convert each row into a dictionary using the function above\n",
    "            # and append the contents to the main collection\n",
    "            content += list(map(parse, t))\n",
    "    \n",
    "    # convert the list of dictionaries to a Pandas dataframe\n",
    "    content = pd.DataFrame([c for c in content if c is not None])\n",
    "    \n",
    "    # write this years worth of weather recordings to csv\n",
    "    content.to_csv(name, index=None)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, I go through the csv contents we generated above. Specific datatypes are enforced to prevent Pandas from dropping leading zeroes, for example, and to make additional operations more streamlined. Each will be explained line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.630Z"
    },
    "code_folding": [
     13,
     58
    ]
   },
   "outputs": [],
   "source": [
    "# get the list of yearly weather files\n",
    "ex_path = os.path.join('.', 'data', 'gsod_extracted')\n",
    "names = [f for f in os.listdir(ex_path) if 'csv' in f]\n",
    "\n",
    "# process each year at a time\n",
    "pbar = tqdm(total=len(names))\n",
    "for name in names:\n",
    "    pbar.set_description(name)\n",
    "    \n",
    "    # load the data, setting data types explicitly or pandas will drop \n",
    "    # the leading zeroes needed for station names. Also, include the \n",
    "    # explicit na values designated in the data documentation\n",
    "    # drop columns we aren't going to use\n",
    "    f1 = pd.read_csv(\n",
    "            os.path.join(ex_path, name), \n",
    "            dtype={\n",
    "                'stn' :'str', \n",
    "                'wban':'str',\n",
    "                'moda':'str',\n",
    "                'frshtt':'str',\n",
    "                'year':'str'},\n",
    "            na_values={\n",
    "                'stn'  :'999999',\n",
    "                'wban' :'99999',\n",
    "                'temp' :'9999.9',\n",
    "                'dewp' :'9999.9',\n",
    "                'slp'  :'9999.9',\n",
    "                'stp'  :'9999.9',\n",
    "                'visib':'999.9',\n",
    "                'wdsp' :'999.9',\n",
    "                'mxspd':'999.9',\n",
    "                'gust' :'999.9',\n",
    "                'max_temp':'9999.9',\n",
    "                'min_temp':'9999.9',\n",
    "                'prcp':'99.9',        \n",
    "                'sndp':'999.9'},\n",
    "        ) \\\n",
    "        .drop(columns=[\n",
    "            'max_temp_flag', 'min_temp_flag', \n",
    "            'temp_cnt', 'dewp_cnt', 'slp_cnt', \n",
    "            'stp_cnt', 'visib_cnt', 'wdsp_cnt'])\n",
    "\n",
    "    # convert the two date columns 'year' and 'moda' to a single pydate\n",
    "    f1['date'] = [\n",
    "        dt.datetime(year=int(r.year), month=int(r.moda[:2]), day=int(r.moda[2:])) \n",
    "        for r in f1.itertuples()\n",
    "    ]\n",
    "    \n",
    "    # extract month number and julian date\n",
    "    f1['month'] = f1.date.apply(lambda x: x.month)\n",
    "    f1['doy'] = f1.date.apply(lambda x: x.timetuple().tm_yday)\n",
    "\n",
    "    # convert prcp values to na where prcp flag is in {'H', 'I'}. see the data docs\n",
    "    f1.loc[(f1.prcp_flag == 'H') | (f1.prcp_flag == 'I'), 'prcp'] = np.nan\n",
    "\n",
    "    # convert 'frshtt' to an ordinal value based on severity where the\n",
    "    # returned value is the number of leading most 1. ie. 010000 -> 2\n",
    "    # 1:fog, 2:rain, 3:snow, 4:hail, 5:thunderstorm, 6:tornado\n",
    "    def fx(x):\n",
    "        x = x[::-1].find('1')\n",
    "        return x if x != -1 else 0\n",
    "\n",
    "    f1['atmos_sev'] = f1.frshtt.apply(fx)\n",
    "    \n",
    "    # create the join key in the same way as we did for weather stations\n",
    "    f1.loc[f1.stn.isnull(), 'stn'] = 'none'\n",
    "    f1.loc[f1.wban.isnull(), 'wban'] = 'none'\n",
    "    f1['key'] = f1.stn + f1.wban\n",
    "    \n",
    "    # perform an inner join with stations\n",
    "    f1 = f1.merge(stations, on='key', how='inner')\n",
    "    \n",
    "    # reorder the columns, dropping the ones that won't be used\n",
    "    prefix = ['lat', 'lon', 'year', 'month', 'doy']\n",
    "    f1 = f1.reindex(columns=prefix + sorted(list(\n",
    "        set(f1.columns) - set(prefix) - {\n",
    "            'moda', 'prcp_flag', 'frshtt', 'stn', 'wban', 'key', 'date'\n",
    "        }\n",
    "    )))\n",
    "    \n",
    "    # write the cleaned dataframe to disk\n",
    "    name = os.path.join(gsod_path, name.replace('.csv', '_cleaned') + '.csv')\n",
    "    f1.to_csv(name, index=None)\n",
    "    \n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single data frame with cleaned values for all years. This generates a dataframe approximately 1.7gb uncompressed which is a significant reduction from the 3.4gb original compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.635Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get the list of cleaned files\n",
    "files = [f for f in os.listdir(gsod_path) if 'cleaned.csv' in f]\n",
    "assert len(files) == 25\n",
    "\n",
    "gsod = pd.concat([pd.read_csv(os.path.join(gsod_path, f)) for f in files])\n",
    "gsod.to_csv(os.path.join('.', 'data', 'gsod.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.639Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanup the temp directories\n",
    "gsod_path = os.path.join('.', 'data', 'gsod')\n",
    "shutil.rmtree(gsod_path)\n",
    "\n",
    "ex_path = os.path.join('.', 'data', 'gsod_extracted')\n",
    "shutil.rmtree(ex_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the fires dataset\n",
    "This dataset comes relatively clean. The only modifications we'll be doing is removing the columns we won't be using, creating a few new, and reordering them for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.643Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate the path and connect to the sqlite fires file\n",
    "path = os.path.join('.', 'data', 'FPA_FOD_20170508.sqlite')\n",
    "conn  = sqlite3.connect(path)\n",
    "\n",
    "# read all the columns we need\n",
    "fires = pd.read_sql_query('''\n",
    "    SELECT FOD_ID,\n",
    "        FIRE_YEAR, DISCOVERY_DOY, DISCOVERY_TIME,\n",
    "        STAT_CAUSE_CODE, CONT_DOY, CONT_TIME,\n",
    "        FIRE_SIZE, LATITUDE, LONGITUDE, OWNER_CODE,\n",
    "        STATE\n",
    "    FROM\n",
    "        Fires;\n",
    "''', conn)\n",
    "\n",
    "# convert column names to lowercase\n",
    "fires.columns = [c.lower() for c in fires.columns]\n",
    "\n",
    "# based on the first 10000 rows, 0.35% have missing containment values which is a \n",
    "# negligible loss at this point in the analysis\n",
    "fires = fires.dropna(subset=[\n",
    "    'discovery_doy', 'discovery_time', 'cont_doy', 'cont_time'\n",
    "], how='any')\n",
    "\n",
    "# convert fire_year, discovery doy, and time to pydate\n",
    "fires['dt_disc'] = [\n",
    "    dt.datetime(year=int(r.fire_year), \n",
    "                month=1, \n",
    "                day=1, \n",
    "                hour=int(r.discovery_time[:2]),\n",
    "                minute=int(r.discovery_time[2:])\n",
    "               ) + \\\n",
    "    dt.timedelta(days=r.discovery_doy)\n",
    "    for r in fires.itertuples()\n",
    "]\n",
    "\n",
    "# convert the containment dates\n",
    "fires['dt_cont'] = [\n",
    "    dt.datetime(year=int(r.fire_year), month=1, day=1, hour=int(r.cont_time[:2]), minute=int(r.cont_time[2:])) + \\\n",
    "    dt.timedelta(days=r.cont_doy)\n",
    "    for r in fires.itertuples()\n",
    "]\n",
    "\n",
    "# create some higher resolution columns\n",
    "def seconds_into_year(x):\n",
    "    a = dt.datetime(year=x.year, month=1, day=1, hour=0, minute=0, second=0)\n",
    "    return int((x-a).total_seconds())\n",
    "\n",
    "def seconds_into_day(x):\n",
    "    a = dt.datetime(year=x.year, month=x.month, day=x.day, hour=0, minute=0, second=0)\n",
    "    return (x-a).seconds\n",
    "\n",
    "# calculate fire duration in seconds, but only if the contained date is\n",
    "# later than the start date\n",
    "fires['disc_soy'] = fires.dt_disc.progress_apply(seconds_into_year)\n",
    "fires['cont_soy'] = fires.dt_cont.progress_apply(seconds_into_year)\n",
    "fires['duration'] = [\n",
    "    r.cont_soy-r.disc_soy \n",
    "    if r.cont_soy > r.disc_soy else np.nan\n",
    "    for r in tqdm(fires.itertuples(), total=len(fires))\n",
    "]\n",
    "\n",
    "# extract month and hour as new columns\n",
    "fires['date']  = fires.dt_disc.progress_apply(lambda x: x.date())\n",
    "fires['month'] = fires.dt_disc.progress_apply(lambda x: x.month)\n",
    "fires['dow']   = fires.dt_disc.progress_apply(lambda x: x.weekday())\n",
    "fires['hod']   = fires.dt_disc.progress_apply(lambda x: x.hour)\n",
    "fires['sod']   = fires.dt_disc.progress_apply(seconds_into_day)\n",
    "\n",
    "# encode the state\n",
    "state_le = LabelEncoder()\n",
    "fires['state'] = state_le.fit_transform(fires.state)\n",
    "\n",
    "# drop some columns we won't be using\n",
    "fires = fires.drop(columns=[\n",
    "    'discovery_time', 'cont_doy', 'cont_time', \n",
    "    'disc_soy', 'cont_soy', 'dt_cont', \n",
    "    'dt_disc'\n",
    "])\n",
    "\n",
    "# rename some columns\n",
    "fires = fires.rename(columns={\n",
    "    'discovery_doy':'doy',\n",
    "    'latitude':'lat',\n",
    "    'longitude':'lon',\n",
    "    'fire_year':'year',\n",
    "    'stat_cause_code':'cause_code',\n",
    "})\n",
    "\n",
    "# reorder the columns\n",
    "prefix = ['fod_id', 'lat', 'lon', 'date', 'year', 'month', 'doy', 'dow', 'hod', 'sod']\n",
    "fires = fires.reindex(columns=prefix + sorted(list(\n",
    "    set(fires.columns) - set(prefix)\n",
    ")))\n",
    "\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible feature we can engineer is the number of nearby fires. You can see the relation by looking at the first couple of rows in the fires table shown above. We see two fires occur on the same day that almost look like duplicates except they're separated by a few kilometers. This can be an especially strong signal for both lightning and arson related fires. \n",
    "\n",
    "This many lookups becomes a compute intensive operation and can take many hours to complete if run iteratively. In the following cell I create subsets of the main fires index. Each subset is sent to a different process where thread pools operate in parallel on the assigned subset. The results are output to separate csv files linked with the fire_id. This precludes transferring the data back from the assigned CPU. Instead, I'll read and join the new feature from disk in the next cell. This compute plan reduced the estimated time to completion from roughly 15 hours to 45 minutes on my local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.647Z"
    },
    "code_folding": [
     50
    ]
   },
   "outputs": [],
   "source": [
    "# thread task\n",
    "def nearby_count(f, qu):\n",
    "    # lookup any fires within 55km\n",
    "    nearby = fires.loc[\n",
    "        (fires.year == f.year) & (fires.doy == f.doy) &\n",
    "        (fires.lat <= f.lat + .25) & (fires.lat >= f.lat - .25) &\n",
    "        (fires.lon <= f.lon + .25) & (fires.lon >= f.lon - .25)\n",
    "    ]\n",
    "    \n",
    "    # update progress\n",
    "    qu.put(1)\n",
    "    \n",
    "    # return the fire id and count\n",
    "    return dict(fod_id=f.fod_id, nearby=len(nearby)-1)\n",
    "\n",
    "# process task\n",
    "def px(batch, fires, start, step, qu):\n",
    "    nearby = list(fires.iloc[start:start+step, :].apply(nearby_count, axis=1, qu=qu))\n",
    "    \n",
    "    path = os.path.join('.', 'data', f'nearby_{batch}.csv')\n",
    "    pd.DataFrame(nearby).to_csv(path, index=None)\n",
    "\n",
    "# number of shards or 'batches'\n",
    "batches = 6    \n",
    "\n",
    "# a container to hold each process\n",
    "processes = []\n",
    "\n",
    "# compute the step size\n",
    "step = int(len(fires)/batches)+1\n",
    "\n",
    "# setup a progress bar and update queue\n",
    "pbar = tqdm(total=len(fires))\n",
    "qu = mul.Queue()\n",
    "\n",
    "# create the subsets and dish out the processor tasks\n",
    "for batch in range(batches):\n",
    "    # calculate the starting point for this subset\n",
    "    start = step*batch\n",
    "    \n",
    "    # create, append, and start the child process\n",
    "    p = mul.Process(target=px, args=(batch, fires, start, step, qu))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "# continue until the children finish\n",
    "complete = False\n",
    "while not complete:\n",
    "    running = batches\n",
    "    \n",
    "    # round robin check of child state\n",
    "    for p in processes:\n",
    "        if not p.is_alive():\n",
    "            running -= 1\n",
    "    \n",
    "    # set completion status if all are finished\n",
    "    if running == 0:\n",
    "        complete = True\n",
    "    \n",
    "    # empty the update qu\n",
    "    while not qu.empty():\n",
    "        t = qu.get()\n",
    "        pbar.update(t)\n",
    "    \n",
    "# terminate and join all the children\n",
    "for p in processes:\n",
    "    p.terminate()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.650Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the batches into one frame\n",
    "path = os.path.join('.', 'data')\n",
    "nearby = pd.concat([\n",
    "    pd.read_csv(os.path.join(path, f)) \n",
    "    for f in os.listdir(path) if 'nearby_' in f\n",
    "], sort=False)\n",
    "\n",
    "# merge with the main and make sure we didn't lose any rows\n",
    "a = len(fires)\n",
    "fires = fires.merge(nearby, on='fod_id', how='inner')\n",
    "assert a == len(fires)\n",
    "\n",
    "# print\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.652Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanup temporary files\n",
    "files = [\n",
    "    os.remove(os.path.join('data', f)) \n",
    "    for f in os.listdir('data') if 'nearby_' in f\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a quick look at the only categorical variable we have - OWNER_CODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T21:44:36.655Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# generate the path and connect to the sqlite fires file\n",
    "path = os.path.join('.', 'data', 'FPA_FOD_20170508.sqlite')\n",
    "conn  = sqlite3.connect(path)\n",
    "\n",
    "# get the mapping of cause codes to description\n",
    "owners = pd.read_sql_query('''\n",
    "    SELECT DISTINCT(OWNER_CODE), OWNER_DESCR\n",
    "    FROM Fires;\n",
    "''', conn)\\\n",
    "    .sort_values('OWNER_CODE')\n",
    "\n",
    "# rename the columns and set the index to code\n",
    "owners = owners.rename(columns={\n",
    "    'OWNER_CODE':'code',\n",
    "    'OWNER_DESCR':'owner'\n",
    "}).set_index('code')\n",
    "\n",
    "# get the counts of each cause\n",
    "bincounts = fires.owner_code.value_counts()\n",
    "\n",
    "# plot\n",
    "iplot(go.Figure(\n",
    "    [go.Bar(\n",
    "        x=[owners.loc[idx].owner for idx in bincounts.index],\n",
    "        y=bincounts,\n",
    "        text=bincounts.index,\n",
    "        textposition='outside'\n",
    "    )],\n",
    "    go.Layout(\n",
    "        title='Distribution of owners',\n",
    "        yaxis=dict(title='Count of owned fires')\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't our target variable but there are clear commonalities we can take advantage of to boost any signal that may come from the responsible land owner. To help understand this a bit better here is the list of federal acronyms:\n",
    "\n",
    "* USFS - United States Forest Service\n",
    "* BIA - Bureau of Indian Affairs\n",
    "* BLM - Bureau of Land Management\n",
    "* NPS - National Park Service\n",
    "* FWS - Fish and Wildlife Service\n",
    "* BOR - Bureau of Reclamation\n",
    "\n",
    "Here is a list of things I notice from the visualization.\n",
    "1. UNDEFINED FEDERAL has very little values and can be combined with OTHER FEDERAL. \n",
    "2. COUNTY owned land can be joined with MUNICIPAL/LOCAL.\n",
    "3. STATE OR PRIVATE can be separted into the STATE and PRIVATE categories. To do this, I'll draw from a random binomial distribution characterized by the ratio between the two.\n",
    "4. TRIBAL can be combined with BIA and I'll rename it to Native American.\n",
    "5. Move the FOREIGN items into MISSING/NOT SPECIFIED.\n",
    "6. Move the MUNICIPAL/LOCAL government owned into STATE owned.\n",
    "7. Group the lower represented federal agencies into the FEDERAL category.\n",
    "\n",
    "This recategorization plan reduces the number of categories from 16 to six which will signficantly boost signal strength and keep the feature space more manageable. I also plan on renaming a few before continuing. Additionally, we'll need to store the new owner descriptions so we preserve the recategorization mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T18:54:35.360615Z",
     "start_time": "2018-12-10T18:54:34.933271Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# recategorize UNDEFINED FEDERAL\n",
    "fires.loc[fires.owner_code == 15, 'owner_code'] = 6\n",
    "\n",
    "# recategorize the under represented federal agencies - BLM, NPS, FWS, BOR\n",
    "fires.loc[[True if o in [1, 3, 4, 10] else False for o in fires.owner_code], 'owner_code'] = 6\n",
    "\n",
    "# rename the federal category\n",
    "owners.loc[6, 'owner'] = 'FEDERAL'\n",
    "\n",
    "# recategorize COUNTY\n",
    "fires.loc[fires.owner_code == 11, 'owner_code'] = 12\n",
    "owners.loc[12, 'owner'] = 'LOCAL'\n",
    "\n",
    "# recategorize STATE OR PRIVATE\n",
    "den = (bincounts[8]+bincounts[7])\n",
    "p = bincounts[8]/den\n",
    "\n",
    "fires.loc[fires.owner_code == 13, 'owner_code'] = np.random.binomial(1, p, len(fires.loc[fires.owner_code == 13]))+7\n",
    "\n",
    "# recategorize TRIBAL\n",
    "fires.loc[fires.owner_code == 9, 'owner_code'] = 2\n",
    "owners.loc[2, 'owner'] = 'NATIVE_AMERICAN'\n",
    "\n",
    "# recategorize FOREIGN\n",
    "fires.loc[fires.owner_code == 0, 'owner_code'] = 14\n",
    "owners.loc[14, 'owner'] = 'OTHER'\n",
    "\n",
    "# recategorize MUNICIPAL/LOCAL\n",
    "fires.loc[fires.owner_code == 12, 'owner_code'] = 7\n",
    "\n",
    "# drop the integer encoding in favor of the new names\n",
    "# create the new column\n",
    "fires['owner'] = 'none'\n",
    "\n",
    "# reformat the owners description to lowercase\n",
    "owners.owner = [o.lower() for o in owners.owner]\n",
    "\n",
    "# assign each code the representative name\n",
    "for code in fires.owner_code.unique():\n",
    "    fires.loc[fires.owner_code == code, 'owner'] = owners.loc[code].owner\n",
    "\n",
    "# drop the original encoded column\n",
    "fires = fires.drop(columns=['owner_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replot the categorical distribution to show the differences we've made for the owner's category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T18:54:35.454227Z",
     "start_time": "2018-12-10T18:54:35.362260Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the counts of each cause\n",
    "bincounts = fires.owner.value_counts()\n",
    "\n",
    "# plot as a bar plot\n",
    "iplot(go.Figure(\n",
    "    [go.Bar(\n",
    "        x=bincounts.index,\n",
    "        y=bincounts,\n",
    "        text=bincounts,\n",
    "        textposition='inside'\n",
    "    )],\n",
    "    go.Layout(\n",
    "        title='Distribution of owners',\n",
    "        yaxis=dict(title='Count of owned fires')\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write the completed fires dataframe to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T18:54:40.257987Z",
     "start_time": "2018-12-10T18:54:35.455670Z"
    }
   },
   "outputs": [],
   "source": [
    "fires.to_csv(os.path.join('.', 'data', 'fires_cleaned.csv'), index=None)\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process ORNL features\n",
    "Each station has a center point and provides the coverage data in both 1km and 50km pixel grids surrounding the station. My first approach to joining the fires and ground cover data was to include any predictions within the station's bounding box but, this led to incredibly sparse results. I leave the cell blocks here to both show my process and why I'm no longer using the data source. In the following cell I load both high and low quality datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T18:57:52.694995Z",
     "start_time": "2018-12-10T18:57:52.667051Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data we'll use, enforce datatypes, and rename columns\n",
    "cover = pd.concat([\n",
    "    pd.read_csv(\n",
    "        os.path.join('.', 'data', f),\n",
    "        usecols=[\n",
    "            'LAT_DD', 'LONG_DD', 'COVR1KM', 'COVR50KM'\n",
    "        ],\n",
    "        dtype={\n",
    "            'COVR1KM':'str',\n",
    "            'COVR50KM':'str'\n",
    "        }\n",
    "    ).rename(columns={\n",
    "        'LAT_DD':'LAT',\n",
    "        'LONG_DD':'LON'\n",
    "    })\n",
    "    for f in [\n",
    "        'EMDI_ClassA_Cover_UMD_81.csv',\n",
    "        'EMDI_ClassB_Cover_UMD_933.csv'\n",
    "    ]\n",
    "], sort=False)\n",
    "    \n",
    "# convert columns to lowercase\n",
    "cover.columns = [c.lower() for c in cover.columns]\n",
    "\n",
    "# create cover 50k grid boundaries\n",
    "cover['lower50_lat'] = cover.lat.apply(lambda x: x-.5)\n",
    "cover['upper50_lat'] = cover.lat.apply(lambda x: x+.5)\n",
    "cover['lower50_lon'] = cover.lon.apply(lambda x: x-.5)\n",
    "cover['upper50_lon'] = cover.lon.apply(lambda x: x+.5)\n",
    "\n",
    "# only include the values within the fire bounding box\n",
    "cover = cover.loc[\n",
    "    (cover.lower50_lat >= min_lat) & (cover.upper50_lat <= max_lat) &\n",
    "    (cover.lower50_lon >= min_lon) & (cover.upper50_lon <= max_lon)\n",
    "]\n",
    "\n",
    "cover.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a sample of fires and the bounding boxes for each station to show just how inadequate the ORNL dataset is. Each point represents a fire with the size of the fire mapped to the size of the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.176822Z",
     "start_time": "2018-12-10T19:02:56.147863Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract a uniform sample of 2k fires\n",
    "sample = fires.sample(1000)\n",
    "\n",
    "# generate scatter plot points\n",
    "fire_trace = go.Scatter(\n",
    "    x=sample.lon, \n",
    "    y=sample.lat,\n",
    "    mode='markers',\n",
    "    marker=dict(        \n",
    "        color='#571C00'\n",
    "    )\n",
    ")\n",
    "\n",
    "# generate the bounding boxes\n",
    "shapes = [\n",
    "    {\n",
    "        'type':'rect',\n",
    "        'x0':r.lower50_lon,\n",
    "        'x1':r.upper50_lon,\n",
    "        'y0':r.lower50_lat,\n",
    "        'y1':r.upper50_lat,\n",
    "        'fillcolor':'rgba(22, 74, 40, .4)',\n",
    "        'line':{\n",
    "            'width':.1\n",
    "        }\n",
    "    }\n",
    "    for r in cover.itertuples()\n",
    "]\n",
    "\n",
    "# plot\n",
    "iplot(go.Figure(\n",
    "    [fire_trace],\n",
    "    layout=go.Layout(\n",
    "        shapes=shapes,\n",
    "        xaxis=dict(\n",
    "            title='longitude',\n",
    "            range=[-125, -78]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='latitude',\n",
    "            range=[25, 58]\n",
    "        ),\n",
    "        title='Ground cover data coverage is insufficient',\n",
    "        width=1200,\n",
    "        height=800\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same goes for soil content because the same stations are used for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.181188Z",
     "start_time": "2018-12-10T19:02:56.147Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "soil = pd.concat([\n",
    "    pd.read_csv(\n",
    "        os.path.join('.', 'data', f)\n",
    "    ).rename(columns={\n",
    "        'LAT_DD':'LAT',\n",
    "        'LONG_DD':'LON'\n",
    "    }).drop(columns='SITE_ID')\n",
    "    for f in [\n",
    "        'EMDI_ClassA_Soil_IGBP_81.csv',\n",
    "        'EMDI_ClassB_Soil_IGBP_933.csv'\n",
    "    ]\n",
    "], sort=False)\n",
    "\n",
    "# convert columns to lowercase\n",
    "soil.columns = [c.lower() for c in soil.columns]\n",
    "\n",
    "# create the station bounding box\n",
    "soil['lower50_lat'] = soil.lat.apply(lambda x: x-.5)\n",
    "soil['upper50_lat'] = soil.lat.apply(lambda x: x+.5)\n",
    "soil['lower50_lon'] = soil.lon.apply(lambda x: x-.5)\n",
    "soil['upper50_lon'] = soil.lon.apply(lambda x: x+.5)\n",
    "\n",
    "# only include the values within the fire bounding box\n",
    "soil = soil.loc[\n",
    "    (soil.lower50_lat >= min_lat) & (soil.upper50_lat <= max_lat) &\n",
    "    (soil.lower50_lon >= min_lon) & (soil.upper50_lon <= max_lon)\n",
    "]\n",
    "\n",
    "soil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.182459Z",
     "start_time": "2018-12-10T19:02:56.148Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# extract a fire sample\n",
    "sample = fires.sample(5000)\n",
    "\n",
    "# generate the fire scatter points\n",
    "fire_trace = go.Scatter(\n",
    "    x=sample.lon, \n",
    "    y=sample.lat,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color='#571C00'\n",
    "    )\n",
    ")\n",
    "\n",
    "shapes = [\n",
    "    {\n",
    "        'type':'rect',\n",
    "        'x0':r.lower50_lon,\n",
    "        'x1':r.upper50_lon,\n",
    "        'y0':r.lower50_lat,\n",
    "        'y1':r.upper50_lat,\n",
    "        'fillcolor':'rgba(22, 74, 40, .4)',\n",
    "        'line':{\n",
    "            'width':.1\n",
    "        }\n",
    "    }\n",
    "    for r in soil.itertuples()\n",
    "]\n",
    "\n",
    "# plot\n",
    "iplot(go.Figure(\n",
    "    [fire_trace],\n",
    "    layout=go.Layout(\n",
    "        shapes=shapes,\n",
    "        xaxis=dict(\n",
    "            title='longitude',\n",
    "            range=[-125, -78]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='latitude',\n",
    "            range=[25, 58]\n",
    "        ),\n",
    "        title='Soil data coverage is insufficient',\n",
    "        width=1200,\n",
    "        height=800\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative data source for land coverage is available for public use. See the [Earth Engine Data Catalog](https://developers.google.com/earth-engine/datasets/catalog/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Generate aggregate weather features associated with each fire\n",
    "We'll need to lookup all reports within a given bounding box centered at the fire's originating location. I use a bounding box to preclude performing pairwise distance lookups which might be more accurate but will incur a significant expense - $O(n^2)$. The embedded hierarchical structure within a degree-decimal formatted coordinate allows us to generate contextually important containment boundaries. The boundaries will include aggregated values from all weather reports $\\pm$ 55.5km of the fire.\n",
    "\n",
    "This is the long running computation may take several days to complete. I wrote it to perform aggregations in batches. Each batch will cache the resulting features to a csv file and continue with the next. Also of note here is that I use a single thread pool rather than the sharding technique to keep memory usage as low as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.183661Z",
     "start_time": "2018-12-10T19:02:56.148Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load cleaned GSOD file\n",
    "gsod = pd.read_csv(os.path.join('.', 'data', 'gsod.csv'))\n",
    "gsod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.184909Z",
     "start_time": "2018-12-10T19:02:56.150Z"
    },
    "code_folding": [
     7
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the cleaned fires\n",
    "fires = pd.read_csv(os.path.join('.', 'data', 'fires_cleaned.csv'), parse_dates=['date'])\n",
    "\n",
    "# start a thread pool and progress bar\n",
    "pool = TPool(mul.cpu_count())\n",
    "pbar = tqdm(total=len(fires))\n",
    "\n",
    "def weather_agg(args):    \n",
    "    try:\n",
    "        # extract the tuple arguments\n",
    "        fod_id, lat, lon, year, doy = args\n",
    "        \n",
    "        # make a copy of the empty record to start this record with\n",
    "        results = empty.copy()\n",
    "        results['fod_id'] = fod_id\n",
    "        \n",
    "        # get all weather reports within 111km\n",
    "        lat_min, lat_max = lat-.5, lat+.5\n",
    "        lon_min, lon_max = lon-.5, lon+.5\n",
    "        \n",
    "        # retrieve all weather reports within the box and 4 days leading up to and including\n",
    "        # the day of the fire\n",
    "        wthr = gsod.loc[\n",
    "            (gsod.lat >= lat_min) & (gsod.lat <= lat_max) &\n",
    "            (gsod.lon >= lon_min) & (gsod.lon <= lon_max) &\n",
    "            (\n",
    "                (gsod.year == year) & (gsod.doy >= doy-4) & (gsod.doy <= doy) |\n",
    "                (gsod.doy <= 4) & (gsod.year == year-1) & (gsod.doy >= 361+doy)\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # get the three day prior aggregates\n",
    "        w_ = wthr.loc[wthr.doy != doy]\n",
    "        if len(w_) > 0:\n",
    "            results['threeDay_atmos_sev'] = np.mean(w_.atmos_sev)\n",
    "            results['threeDay_temp_max']  = np.max(w_.temp_max)\n",
    "            results['threeDay_temp_min']  = np.min(w_.temp_min)\n",
    "            results['threeDay_temp']  = np.median(w_.temp)\n",
    "            results['threeDay_sndp']  = np.median(w_.sndp)\n",
    "            results['threeDay_dewp']  = np.median(w_.dewp)\n",
    "            results['threeDay_gust']  = np.max(w_.gust)\n",
    "            results['threeDay_mxspd'] = np.max(w_.mxspd)\n",
    "            results['threeDay_stp']   = np.median(w_.stp)\n",
    "            results['threeDay_temp']  = np.median(w_.temp)\n",
    "            results['threeDay_slp']   = np.median(w_.slp)\n",
    "            results['threeDay_wdsp']  = np.median(w_.wdsp)\n",
    "            results['threeDay_prcp']  = np.sum(w_.prcp)\n",
    "            results['threeDay_visib'] = np.median(w_.visib)   \n",
    "\n",
    "        # get the dayOf aggregates   \n",
    "        w_ = wthr.loc[wthr.doy == doy]\n",
    "        if len(w_) > 0:            \n",
    "            results['dayOf_atmos_sev'] = np.mean(w_.atmos_sev)\n",
    "            results['dayOf_temp_max']  = np.max(w_.temp_max)\n",
    "            results['dayOf_temp_min']  = np.min(w_.temp_min)\n",
    "            results['dayOf_temp']  = np.median(w_.temp)\n",
    "            results['dayOf_sndp']  = np.median(w_.sndp)\n",
    "            results['dayOf_dewp']  = np.median(w_.dewp)\n",
    "            results['dayOf_gust']  = np.max(w_.gust)\n",
    "            results['dayOf_mxspd'] = np.max(w_.mxspd)\n",
    "            results['dayOf_stp']   = np.median(w_.stp)\n",
    "            results['dayOf_temp']  = np.median(w_.temp)\n",
    "            results['dayOf_slp']   = np.median(w_.slp)\n",
    "            results['dayOf_wdsp']  = np.median(w_.wdsp)\n",
    "            results['dayOf_prcp']  = np.median(w_.prcp)\n",
    "            results['dayOf_visib'] = np.median(w_.visib)   \n",
    "    \n",
    "    # catch all exceptions and continue gracefully but make sure we\n",
    "    # notify in case any occur\n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        print(exc_type, e, exc_tb.tb_lineno)\n",
    "    \n",
    "    pbar.update(1)\n",
    "    return results\n",
    "\n",
    "# create the dayOf columns\n",
    "excols = {'lat', 'lon', 'elev', 'year', 'month', 'doy', 'fod_id'}\n",
    "daily_cols    = ['dayOf_'    + c for c in list(set(gsod.columns) - excols)]\n",
    "threeDay_cols = ['threeDay_' + c for c in list(set(gsod.columns) - excols)]\n",
    "\n",
    "# create an empty dictionary to start each feature row\n",
    "empty = dict()\n",
    "for c in daily_cols+threeDay_cols:\n",
    "    empty[c] = np.nan\n",
    "\n",
    "fires_temp = os.path.join('.', 'data', 'fires')\n",
    "if not os.path.exists(fires_temp):\n",
    "    os.mkdir(fires_temp)\n",
    "    \n",
    "# perform this operation in batches caching the fire results each iteration\n",
    "start, step = 0, 10000\n",
    "for i in range(0, len(fires), step):\n",
    "    # get the set of indices to process\n",
    "    idx_set = fires.index.tolist()[i:i+step]\n",
    "    \n",
    "    # process\n",
    "    batch = pool.map(weather_agg, [\n",
    "        (r.fod_id, r.lat, r.lon, r.year, r.doy) \n",
    "        for r in fires.loc[idx_set].itertuples()\n",
    "    ])\n",
    "    \n",
    "    # cache\n",
    "    pd.DataFrame(batch).to_csv(os.path.join('.', 'data', 'fires', f'fires_b{i}.csv'), index=None)\n",
    "    \n",
    "pool.close(); pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, read all batches into a single dataframe and write it back to disk as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.186084Z",
     "start_time": "2018-12-10T19:02:56.151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# combine the batches into a single dataframe\n",
    "path = os.path.join('.', 'data', 'fires')\n",
    "fire_weather = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(os.path.join(path, f)) \n",
    "        for f in os.listdir(path) if '.csv' in f\n",
    "    ],\n",
    "    sort=False\n",
    ")\n",
    "\n",
    "# write the combined dataframe to disk\n",
    "path = os.path.join('.', 'data', 'fire_weather.csv')\n",
    "fire_weather.to_csv(path, index=None)\n",
    "\n",
    "# clean the temp dir\n",
    "shutil.rmtree(fires_temp)\n",
    "\n",
    "fire_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the combined file to use for analysis and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.187200Z",
     "start_time": "2018-12-10T19:02:56.152Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the cleaned fires data\n",
    "path = os.path.join('.', 'data', 'fires_cleaned.csv')\n",
    "fires = pd.read_csv(path, parse_dates=['date'])\n",
    "\n",
    "# load the weather aggregations\n",
    "path = os.path.join('.', 'data', 'fire_weather.csv')\n",
    "weather = pd.read_csv(path)\n",
    "\n",
    "# merge the dataframes on the fod_id\n",
    "df = fires.merge(weather, on='fod_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.188339Z",
     "start_time": "2018-12-10T19:02:56.153Z"
    }
   },
   "outputs": [],
   "source": [
    "def nan_percentages(df, show_zero=False):\n",
    "    cols = sorted(df.columns)\n",
    "    d, p = len(df), {}\n",
    "\n",
    "    for col in cols:\n",
    "        a = sum(pd.isnull(df[col]))\n",
    "        p[col] = a/d\n",
    "\n",
    "    for k, v in p.items():\n",
    "        n = len(k) if len(k) <= 20 else 20\n",
    "        v = np.round(v, 4)\n",
    "        \n",
    "        if v != 0 or show_zero:\n",
    "            print('{:<20} {:<5}'.format(k[:n], v))\n",
    "\n",
    "compute_cols = list(set(df.columns) - {'fod_id', 'date'})        \n",
    "nan_percentages(df[compute_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, we notice that nearly 13% of our rows weren't recorded correctly. Those are the records where the contanment date was recorded before the discovery date. Let's drop those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.189381Z",
     "start_time": "2018-12-10T19:02:56.154Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[[not b for b in df.duration.isnull()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T21:26:26.042079Z",
     "start_time": "2018-12-04T21:26:25.800951Z"
    }
   },
   "source": [
    "We have quite a few NA values in the resulting weather data and I'm running out of time to do any complex fixes. For the purposes of this project we're going to make some quick assumptions and transformations. Lets see how much of the dataset doesn't have any dayOf features at all.\n",
    "\n",
    "note: using the full datasets removes 25.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.190370Z",
     "start_time": "2018-12-10T19:02:56.155Z"
    }
   },
   "outputs": [],
   "source": [
    "np.round(len(df.loc[\n",
    "    df.dayOf_prcp.isnull() &\n",
    "    df.dayOf_visib.isnull() &\n",
    "    df.dayOf_gust.isnull() &\n",
    "    df.dayOf_dewp.isnull() & \n",
    "    df.dayOf_temp_max.isnull() &\n",
    "    df.dayOf_temp_min.isnull() &\n",
    "    df.dayOf_temp.isnull() &\n",
    "    df.dayOf_atmos_sev.isnull() &\n",
    "    df.dayOf_wdsp.isnull() &\n",
    "    df.dayOf_mxspd.isnull()\n",
    "])/len(df)*100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite a high percentage and accounts for many of the missing values. Lets drop those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.191475Z",
     "start_time": "2018-12-10T19:02:56.156Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\n",
    "    'dayOf_prcp', 'dayOf_visib', 'dayOf_gust', 'dayOf_dewp',\n",
    "    'dayOf_temp_max', 'dayOf_temp_min', 'dayOf_temp',\n",
    "    'dayOf_atmos_sev', 'dayOf_wdsp', 'dayOf_mxspd'\n",
    "], how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets look at sndp - snow depth. This column is almost completely nan but we don't have to lose the information. Lets transform this column into an indicator that simply says whether or not snow was present at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.192518Z",
     "start_time": "2018-12-10T19:02:56.158Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the indicators\n",
    "df['threeDay_snow'] = [1 if not b else 0 for b in df.threeDay_sndp.isnull()]\n",
    "df['dayOf_snow']    = [1 if not b else 0 for b in df.dayOf_sndp.isnull()]\n",
    "\n",
    "# drop the original\n",
    "df = df.drop(columns=['threeDay_sndp', 'dayOf_sndp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next highest source of missing values is in our pressure columns: slp and stp. I'm going to drop these columns all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.193671Z",
     "start_time": "2018-12-10T19:02:56.159Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the pressure columns\n",
    "df = df.drop(columns=[\n",
    "    'dayOf_stp', 'dayOf_slp', 'threeDay_stp', 'threeDay_slp'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take the missing gust values. For this, lets just take the maximum recorded windspeed for the day and three day respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.194829Z",
     "start_time": "2018-12-10T19:02:56.159Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.dayOf_gust.isnull(), 'dayOf_gust'] = df.loc[df.dayOf_gust.isnull(), 'dayOf_mxspd']\n",
    "df.loc[df.threeDay_gust.isnull(), 'threeDay_gust'] = df.loc[df.threeDay_gust.isnull(), 'threeDay_mxspd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use linear regression models to impute any of the remaining missing values. In the next cell, I loop through each collumn with missing values generating a model for each. I use these individual models to predict the remaining missing values. This preserves any existing relationship that may exist between the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.195880Z",
     "start_time": "2018-12-10T19:02:56.161Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the remaining columns with nan values\n",
    "to_impute = [c for c in df.columns if sum(df.loc[:, c].isnull()) > 0]\n",
    "\n",
    "# make sure we don't use these columns in the regression model\n",
    "excluded_columns = {\n",
    "    'fod_id', 'date', 'year', 'sod', 'cause_code', \n",
    "    'duration', 'fire_size', 'owner',\n",
    "}\n",
    "\n",
    "# impute each remaining missing value\n",
    "for c in tqdm(to_impute):\n",
    "    # extract the rows that need imputed\n",
    "    x = df[[not b for b in df.loc[:, c].isnull()]]\n",
    "    \n",
    "    # get the column names to use \n",
    "    inputs = set(df.columns) - excluded_columns - {c}\n",
    "    \n",
    "    # create the r-style formula\n",
    "    formula = c + '~' + '+'.join(inputs)\n",
    "    \n",
    "    # build and fit the model\n",
    "    model = smf.ols(formula=formula, data=df).fit()\n",
    "    \n",
    "    # make predictions\n",
    "    predictions = model.predict(exog=df.loc[\n",
    "        df.loc[:, c].isnull(), inputs\n",
    "    ])\n",
    "    \n",
    "    # ensure predictions aren't negative\n",
    "    predictions = [p if p > 0 else 0 for p in predictions]\n",
    "    \n",
    "    # set the missing vals to the predicted\n",
    "    df.loc[df.loc[:, c].isnull(), c] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final check lets print the percentage of nan values to make sure we've generated a complete dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.196952Z",
     "start_time": "2018-12-10T19:02:56.162Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_cols = list(set(df.columns) - {'fod_id', 'date'})    \n",
    "nan_percentages(df[compute_cols], show_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.197970Z",
     "start_time": "2018-12-10T19:02:56.164Z"
    }
   },
   "outputs": [],
   "source": [
    "# write it to disk\n",
    "path = os.path.join('.', 'data', 'fires_complete.csv')\n",
    "df.to_csv(path, index=None)\n",
    "\n",
    "# show it\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the research questions we need to take a look at the feature correlations and build a model to assess how much information each feature provides.\n",
    "\n",
    "Before we build the model let's get an idea of both feature correlations and the distribution of classes in the dataset. We'll check feature correlations by utilizing the Pandas corr function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.198991Z",
     "start_time": "2018-12-10T19:02:56.164Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the cleaned fire data\n",
    "path = os.path.join('.', 'data', 'fires_complete.csv')\n",
    "df = pd.read_csv(\n",
    "    path, \n",
    "    parse_dates=['date'],\n",
    "    dtype={'fod_id':'object'}\n",
    ")\n",
    "\n",
    "# convert owners to indicator variables\n",
    "df = pd.concat([df, pd.get_dummies(df.owner)], axis=1).drop(columns='owner')\n",
    "\n",
    "# rename the Native American col\n",
    "df = df.rename(columns={'native american':'native_american'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.200018Z",
     "start_time": "2018-12-10T19:02:56.165Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the columns we'll be training on\n",
    "indicator_cols = ['federal', 'native_american', 'other', 'private', 'state', 'usfs']\n",
    "numeric_cols = sorted(list(set(df.columns) - {'fod_id', 'date', 'cause_code'} - set(indicator_cols)))\n",
    "compute_cols = indicator_cols + numeric_cols\n",
    "\n",
    "iplot(go.Figure(\n",
    "    [go.Heatmap(\n",
    "        x=compute_cols,\n",
    "        y=compute_cols,\n",
    "        z=np.array(df.loc[:, compute_cols].corr()),\n",
    "        colorscale='RdBu',\n",
    "        zmin=-1,\n",
    "        zmax=1\n",
    "    )],\n",
    "    go.Layout(\n",
    "        title='Correlation plot',\n",
    "        height=800,\n",
    "        width=800\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T21:07:21.955510Z",
     "start_time": "2018-12-04T21:07:21.948499Z"
    }
   },
   "source": [
    "The first I noticed from this plot is the 4 high and positively correlated variables. These are gust and max windspeed for both the day of and three day variables. Not only are they going to be naturally correlated but we used the max speed to impute missing gust values.\n",
    "\n",
    "An interesting correlation exists between nearby and Native American lands. It appears as if Native American lands tend to have more simultaneous wildfires. \n",
    "\n",
    "And to see the distribution of classes we'll query the distinct cause descriptions for the original fires dataset and visualizing the counts of each.\n",
    "\n",
    "In Part 1 we left the dataframe in a more dense form by not expanding the categorical variables. We only have one of those features: owner code. Lets go ahead and convert them before proceeding any further. First, I query the orginal fires dataset to get the mapping of owner code to description to give us a bit more context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.201192Z",
     "start_time": "2018-12-10T19:02:56.166Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate the path and connect to the sqlite fires file\n",
    "path = os.path.join('.', 'data', 'FPA_FOD_20170508.sqlite')\n",
    "conn  = sqlite3.connect(path)\n",
    "\n",
    "# get the mapping of cause codes to description\n",
    "cause_map = pd.read_sql_query('''\n",
    "    SELECT DISTINCT(STAT_CAUSE_CODE), STAT_CAUSE_DESCR\n",
    "    FROM Fires;\n",
    "''', conn)\\\n",
    "    .sort_values('STAT_CAUSE_CODE')\n",
    "\n",
    "# rename the columns and set the index to code\n",
    "cause_map = cause_map.rename(columns={\n",
    "    'STAT_CAUSE_CODE':'code',\n",
    "    'STAT_CAUSE_DESCR':'cause'\n",
    "}).set_index('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.202250Z",
     "start_time": "2018-12-10T19:02:56.167Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get the counts of each cause\n",
    "bincounts = df.cause_code.value_counts()\n",
    "\n",
    "# plot as a bar plot\n",
    "iplot(go.Figure(\n",
    "    [go.Bar(\n",
    "        x=[cause_map.loc[idx].cause for idx in bincounts.index],\n",
    "        y=bincounts,\n",
    "        text=bincounts.index,\n",
    "        textposition='outside'\n",
    "    )],\n",
    "    go.Layout(\n",
    "        title='Distribution of causes is not uniformly distributed',\n",
    "        yaxis=dict(title='Count of fires')\n",
    "    )\n",
    "), filename='wildfires_class_distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this visualization we see difficulties beginning to form. The classes are far from uniformly distributed which makes predicting the lower represented classes more difficult.\n",
    "\n",
    "The classification model I'm going to use for this project is the [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBM) implemented by scikit-learn. This model allows for fine tuning between bias and variance and works well with imbalaned datasets. I follow the tuning procedures written [here](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/) to find the best performing hyperparameters.\n",
    "\n",
    "In the next column, I extract the training features. The target variable, cause_code, is also extracted. Finally, the data is split into training and validation sets. We have plenty of rows so I use 90% of the data for training. Also of note is that I stratify the training samples in an effort to keep a more appropriate balance between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.203417Z",
     "start_time": "2018-12-10T19:02:56.168Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# extract the columns we'll use for prediction\n",
    "X = df.loc[:, compute_cols]\n",
    "\n",
    "# extract the target variable\n",
    "y = np.array(df.cause_code)\n",
    "\n",
    "# perform the stratified train-test split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.9, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to tuning a GBM is to find the optimal learning rate. This is performed with a 5-fold cross-validation split. For now, I set the min samples, max depth, and subsample to an approximate amount as described in the blog. These will be tuned in later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.204510Z",
     "start_time": "2018-12-10T19:02:56.169Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the parameter grid for tuning\n",
    "params = {\n",
    "    'learning_rate':np.linspace(0.05, .2, 3),\n",
    "}\n",
    "\n",
    "# setup the cross-validation scheme\n",
    "cv = GridSearchCV(\n",
    "    GradientBoostingClassifier(\n",
    "        max_features='sqrt',\n",
    "        min_samples_split=400,\n",
    "        min_samples_leaf=25,\n",
    "        max_depth=10,\n",
    "        subsample=.8\n",
    "    ), \n",
    "    params, \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# fit\n",
    "cv = cv.fit(X, y)\n",
    "\n",
    "# print the best parameters and score\n",
    "cv.best_params_, cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's find the optimal number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.205613Z",
     "start_time": "2018-12-10T19:02:56.170Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the parameter grid for tuning\n",
    "params = {\n",
    "    'n_estimators':range(20, 80, 10)\n",
    "}\n",
    "\n",
    "# setup the cross-validation scheme\n",
    "cv = GridSearchCV(\n",
    "    GradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_features='sqrt',\n",
    "        min_samples_split=400,\n",
    "        min_samples_leaf=25,\n",
    "        max_depth=10,\n",
    "        subsample=.8\n",
    "    ), \n",
    "    params, \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# fit\n",
    "cv = cv.fit(X, y)\n",
    "\n",
    "# print the best parameters and score\n",
    "cv.best_params_, cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's tune the breadth and depth of each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.206589Z",
     "start_time": "2018-12-10T19:02:56.171Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the parameter grid for tuning\n",
    "params = {\n",
    "    'max_depth':range(5, 16, 2),\n",
    "    'min_samples_split':range(100, 600, 100),\n",
    "}\n",
    "\n",
    "# setup the cross-validation scheme\n",
    "cv = GridSearchCV(\n",
    "    GradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=70,\n",
    "        max_features='sqrt',\n",
    "        min_samples_leaf=25,\n",
    "        subsample=.8\n",
    "    ), \n",
    "    params, \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# fit\n",
    "cv = cv.fit(X, y)\n",
    "\n",
    "# print the best parameters and score\n",
    "cv.best_params_, cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our best parameters, let's refit using the new parameters and validate our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.207597Z",
     "start_time": "2018-12-10T19:02:56.172Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# build the classifier\n",
    "gbm = GradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=70,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=25,\n",
    "    subsample=0.8,\n",
    "    max_depth=13,\n",
    "    min_samples_split=300\n",
    ")\n",
    "\n",
    "# fit\n",
    "gbm = gbm.fit(X, y)\n",
    "\n",
    "# predict and show accuracy on the validation set\n",
    "pred = gbm.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.208618Z",
     "start_time": "2018-12-10T19:02:56.173Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(pred == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.209826Z",
     "start_time": "2018-12-10T19:02:56.174Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "data = sorted(\n",
    "    list(zip(compute_cols, gbm.feature_importances_)),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "iplot(go.Figure(\n",
    "    [go.Bar(\n",
    "        x=[xi[0] for xi in data],\n",
    "        y=[xi[1] for xi in data]\n",
    "    )],\n",
    "    go.Layout(\n",
    "        title='Rank of Feature Importance'\n",
    "    )\n",
    "), filename='wildfires_feature_importance_all_classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy we could get is approximately 58% (52% for the sample set) on the validation set. This isn't horrible given the number and balance of classes. However, we can do better. The vegetation and soil data could be a welcomed addition to the model and there's no doubt we could engineer more features off the existing. For now, this will have to do. The feature importances are shown visually above. These were what the decision tree model deemed as most important for infering the cause of a wildfire. \n",
    "\n",
    "It's a bit unfortunate to see the weather features not performing very well. I expected the day of windspeed, visibility, and temperature to give us some information. Lets take a look at their distributions by cause code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.210850Z",
     "start_time": "2018-12-10T19:02:56.175Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a list of lists containing the wind speeds by cause code\n",
    "wdsp = [\n",
    "    df.loc[df.cause_code == c].sample(1000, replace=True).dayOf_wdsp.tolist() \n",
    "    for c in cause_map.index\n",
    "]\n",
    "\n",
    "# create the figure\n",
    "fig = ff.create_distplot(\n",
    "    wdsp,\n",
    "    cause_map.cause.tolist(),\n",
    "    show_rug=False\n",
    ")\n",
    "\n",
    "# update the layout\n",
    "fig['layout'].update(\n",
    "    title='Density of Windspeed by Wildfire Cause',\n",
    "    xaxis=dict(title='Windspeed'),\n",
    "    yaxis=dict(title='Density')\n",
    ")\n",
    "\n",
    "# plot\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the distributions aren't separable so using them for inference was a bit of a waste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Feature Importance visual we see that longitude coordinates turns up as the second most important feature. This isn't suprising when you take a look at side-by-side boxplots of each cause and longitude (displayed in the next cell). Lightning is the most occuring reason for wildfires and has the tightest IQR. On the same note you can how vastly different the placement of debris burning (2nd highest cause) is compared to lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.212003Z",
     "start_time": "2018-12-10T19:02:56.177Z"
    },
    "code_folding": [
     44,
     47
    ]
   },
   "outputs": [],
   "source": [
    "# create a boxplot for each cause code\n",
    "traces = []\n",
    "for c in df.cause_code.unique():\n",
    "    lont = go.Box(\n",
    "        name=cause_map.loc[c].cause,\n",
    "        y=df.loc[df.cause_code == c].lon,\n",
    "        showlegend=False,\n",
    "        marker=dict(color='#262A3D'),\n",
    "        boxpoints=False,\n",
    "    )\n",
    "    traces.append(lont)\n",
    "\n",
    "# plot the boxes\n",
    "iplot(go.Figure(\n",
    "    traces,\n",
    "    # add the global median line to make comparisons easier\n",
    "    layout=go.Layout(\n",
    "        shapes=[{\n",
    "            'type':'line',\n",
    "            'x0':-0.5,\n",
    "            'x1':13.5,\n",
    "            'y0':df.lon.median(),\n",
    "            'y1':df.lon.median(),\n",
    "            'line':{\n",
    "                'color':'#75684A',\n",
    "                'dash':'dash'\n",
    "            }\n",
    "        }],\n",
    "        # annotate the median line\n",
    "        annotations=[{\n",
    "            'x':13,\n",
    "            'y':df.lon.mean()+5,\n",
    "            'text':'median',\n",
    "            'ax':0,\n",
    "            'ay':-10\n",
    "        }],\n",
    "        title='Longitude by Wildfire Cause',\n",
    "        height=400\n",
    "    )))\n",
    "\n",
    "# extract a subset so we don't overload the browser\n",
    "df_ = df.loc[(df.lat < 55) & (df.lon > -130) & (df.lat > 20)].sample(20000)\n",
    "\n",
    "# draw the plot\n",
    "iplot(go.Figure(\n",
    "    [\n",
    "        # add lightning to the plot\n",
    "        go.Scatter(\n",
    "            name='lightning',\n",
    "            x=df_.loc[df.cause_code==1].lon,\n",
    "            y=df_.loc[df.cause_code==1].lat,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=4,\n",
    "                opacity=.5\n",
    "            )\n",
    "        ),\n",
    "        # add debris burning to the plot\n",
    "        go.Scatter(\n",
    "            name='debris burning',\n",
    "            x=df_.loc[df.cause_code==5].lon,\n",
    "            y=df_.loc[df.cause_code==5].lat,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=4,\n",
    "                opacity=.5\n",
    "            )\n",
    "        ),\n",
    "        # add arson to the plot\n",
    "        go.Scatter(\n",
    "            name='arson',\n",
    "            x=df_.loc[df.cause_code==7].lon,\n",
    "            y=df_.loc[df.cause_code==7].lat,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=4,\n",
    "                opacity=.5\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    go.Layout(\n",
    "        title='Wildfires by cause',\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        legend=dict(\n",
    "            orientation='h',\n",
    "            xanchor='center',\n",
    "            x=.5,\n",
    "            y=1.05\n",
    "        ),\n",
    "        height=600,\n",
    "        width=1000\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a bit more clearly how debris burning seems to be a problem in the southern states of country. Another interesting note from the box and scatter plots is how the concentration of arson implemented wildfires is also aggregated towards the South Eastern side of our country. Lightning is having the somewhate obvious increased impact in the drier climates out west.\n",
    "\n",
    "Another feature of high importance is doy - day of year. We have a very clear fire season but this doesn't necessarily translate to discernability among causes it just makes it easier to start a fire in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.213171Z",
     "start_time": "2018-12-10T19:02:56.178Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate a rolling mean with a five day window of \n",
    "# the count of each fire cause in each window\n",
    "df_ = df[['cause_code', 'doy', 'fod_id', 'dayOf_temp', 'dayOf_dewp']]\\\n",
    "    .sort_values(by='doy')\\\n",
    "    .groupby(['cause_code', 'doy'])\\\n",
    "    .count().rolling(7).mean()\\\n",
    "    .reset_index()\n",
    "\n",
    "# create a filled scatter plot for each cause\n",
    "traces = []\n",
    "for c in df.cause_code.unique():\n",
    "    trace = go.Scatter(\n",
    "        name=cause_map.loc[c].cause,\n",
    "        x=df_.loc[df_.cause_code == c].doy,\n",
    "        y=df_.loc[df_.cause_code == c].fod_id,\n",
    "        mode='lines',\n",
    "        fill='tozeroy'\n",
    "        \n",
    "    )\n",
    "    traces.append(trace)\n",
    "    \n",
    "# create labels and tick positions for the xaxis\n",
    "labels = [calendar.month_name[i] for i in range(0, 13, 2)]\n",
    "tickvals = [i*30.5-15 for i in range(0, 12, 2)]\n",
    "    \n",
    "# plot\n",
    "iplot(go.Figure(\n",
    "    traces, \n",
    "    layout=go.Layout(\n",
    "        title='The Seasonality of Wildfire Causes',\n",
    "        height=500,\n",
    "        xaxis=go.layout.XAxis(\n",
    "            ticktext=labels,\n",
    "            tickvals=tickvals,\n",
    "            title='Month of Year'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Count of Fires'\n",
    "        )\n",
    "    )\n",
    "), filename='The Seasonality of Wildfire Causes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that not only is there a season for lightning started wildfires but there's a season for debris burning and arsonists as well. I was quite surprised to see this visualization. Another interesting feature of the visualization that somewhat validates the data is the spike in firework caused wildfires right near the beginning of July. To note two more spikes in density double click on 'children' in the legend. You can click or double click the legend to isolate specific causes. When filtered to just children you notice two clear spikes; one occurs near Independence day but the other and much larger was a surprising find. Children seem to the most of their damage right around the Spring Break time in March. All the other causes follow the general trend of the drying climate during summer.\n",
    "\n",
    "We notice some issues with data when clicking through the legend showing one cause at a time. We can see the spike of miscellaneous fires near July 4th as well which indicates that many of those may have been missclassified. The same can be said for campfire, smoking, and missing/undefined.\n",
    "\n",
    "Lets take a quick look at the count of nearby fires feature we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.214285Z",
     "start_time": "2018-12-10T19:02:56.179Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create a boxplot for each cause code\n",
    "df_ = df[['cause_code', 'nearby']].groupby('cause_code').sum().reset_index().sort_values(by='nearby', ascending=False)\n",
    "\n",
    "trace = go.Bar(\n",
    "    x=[cause_map.loc[c].cause for c in df_.cause_code.unique()],\n",
    "    y=df_.nearby\n",
    ")\n",
    "\n",
    "iplot(go.Figure([trace], layout=dict(title='Lightning and Arson')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gathered some interesting information from the model but 58% accuracy isn't incredibly reliable. I'd like to predict for just arson related fires as it would be useful and allow us to isolate a single signal thereby amplifying our prediction accuracy.\n",
    "\n",
    "To begin creating the arson model we need to reset the cause codes to a binary label with 1 being arson and everything else 0. I will also drop the bottom three features as they provided no predictive power in the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.333888Z",
     "start_time": "2018-12-10T19:02:56.317807Z"
    }
   },
   "outputs": [],
   "source": [
    "# reset the labels\n",
    "df['arson'] = df.cause_code.apply(lambda x: int(x == 7))\n",
    "\n",
    "# get the list of features to use\n",
    "cols = list(set(compute_cols) - {'state', 'dayOf_snow', 'threeDay_snow'}) + ['arson']\n",
    "\n",
    "# extract those features\n",
    "X = df.loc[:, cols].replace([np.inf, -np.inf], np.nan).dropna().sample(int(len(df)*.5))\n",
    "y = np.array(X.arson)\n",
    "\n",
    "# drop the target label\n",
    "X = X.drop(columns=['arson']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the next column several times performing the same procedure as above to tune the model. What you see here is the last step of the tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.336323Z",
     "start_time": "2018-12-10T19:02:56.180Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    #'learning_rate':np.linspace(0.05, .2, 5),\n",
    "    #'n_estimators':range(20, 100, 10),\n",
    "    'max_depth':range(5, 16, 2),\n",
    "    'min_samples_leaf':range(10, 50, 10),\n",
    "    'min_samples_split':range(100, 400, 100),\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(\n",
    "    GradientBoostingClassifier(\n",
    "        max_features='sqrt',\n",
    "        learning_rate=0.0875,\n",
    "        n_estimators=90,\n",
    "        #min_samples_split=200,\n",
    "        #min_samples_leaf=20,\n",
    "        #max_depth=15,\n",
    "        subsample=.8\n",
    "    ), \n",
    "    params, \n",
    "    cv=3, \n",
    "    n_jobs=-1, \n",
    "    verbose=10\n",
    ")\n",
    "cv = cv.fit(X, y)\n",
    "\n",
    "# print the best parameters and score\n",
    "cv.best_params_, cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the final model a stratified KFold to see our generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.337954Z",
     "start_time": "2018-12-10T19:02:56.182Z"
    }
   },
   "outputs": [],
   "source": [
    "# set number of folds and start a cross val\n",
    "folds = 3\n",
    "cv = StratifiedKFold(n_splits=folds)\n",
    "\n",
    "# generate the model\n",
    "gba = GradientBoostingClassifier(\n",
    "    learning_rate=.0875,\n",
    "    max_depth=15,\n",
    "    min_samples_split=100,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=40,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "# metric containers\n",
    "tprs, aucs, fprs, tprs_ = [], [], [], []\n",
    "mean_fprs = np.linspace(0, 1, 100)\n",
    "traces = []\n",
    "\n",
    "# fit each fold and gather performance metrics\n",
    "for train, test in tqdm(cv.split(X, y), total=folds):\n",
    "    # fit and predict \n",
    "    probas = gba.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    \n",
    "    # gather metrics\n",
    "    fpr, tpr, _ = roc_curve(y[test], probas[:, 1])\n",
    "    fprs.append(fpr)\n",
    "    tprs_.append(tpr)\n",
    "    tprs.append(interp(mean_fprs, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    \n",
    "    # get AUC score\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.339401Z",
     "start_time": "2018-12-10T19:02:56.183Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate the ROC plot, adding a curve for each fold\n",
    "traces = []\n",
    "for i, t in enumerate(zip(tprs, fprs, aucs)):\n",
    "    tpr, fpr, auc = t\n",
    "    traces.append(go.Scatter(\n",
    "        name=f'ROC fold {i+1} (AUC: {np.round(auc, 2)})',\n",
    "        x=mean_fprs,\n",
    "        y=tpr,\n",
    "        mode='lines',\n",
    "    ))\n",
    "\n",
    "# add the x=y line\n",
    "traces.append(\n",
    "    go.Scatter(\n",
    "        x=[0,1],\n",
    "        y=[0,1],\n",
    "        mode='lines',\n",
    "        line=dict(\n",
    "            dash='dash',\n",
    "            color='red'\n",
    "        ),\n",
    "        showlegend=False\n",
    "    )\n",
    ") \n",
    "\n",
    "# plot\n",
    "iplot(go.Figure(traces, go.Layout(\n",
    "    title='ROC',\n",
    "    titlefont=dict(size=22),\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        xanchor='center',\n",
    "        x=0.5,\n",
    "        y=1.05\n",
    "    ),\n",
    "    xaxis=dict(title='FPR'),\n",
    "    yaxis=dict(title='TPR')\n",
    ")), filename='wildfires_arson_roc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm glad to see that the arson only model is performing relatively well when predicting whether or not a wildfire has been started by an arsonist. I have no doubt that we could engineer more features and bring the vegetation data to achieve a higher AUC. Lets see how the most important features compare to the multiclass model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.340500Z",
     "start_time": "2018-12-10T19:02:56.184Z"
    }
   },
   "outputs": [],
   "source": [
    "# zip and sort the feature names and importances\n",
    "data = sorted(\n",
    "    list(zip(compute_cols, gba.feature_importances_)),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# generate the plot\n",
    "iplot(go.Figure(\n",
    "    [go.Bar(\n",
    "        x=[xi[0] for xi in data],\n",
    "        y=[xi[1] for xi in data]\n",
    "    )],\n",
    "    go.Layout(\n",
    "        title='Rank of Feature Importance for the Arson-Only Model'\n",
    "    )\n",
    "), filename='wildfires_feature_importance_all_classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see interesting avenues open for further exploration. Arsonists apparently work at specific times of days, and they're heavily  affected by the maximum temperature of the day. Somewhat of less importance is federal lands but I'm curious as to whether they burn them more or less and if so, is it statistically significant?\n",
    "\n",
    "I'll quickly answer these questions by plotting the sod by number of fires and a fitting a logistic regression model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T19:02:56.341784Z",
     "start_time": "2018-12-10T19:02:56.185Z"
    },
    "code_folding": [
     7,
     17
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the r-style formula\n",
    "formula = 'arson ~ sod + threeDay_temp_max + federal + threeDay_atmos_sev'\n",
    "\n",
    "# take a stratified sample so the model doesn't overfit towards either label.\n",
    "df_ = df.groupby('arson').apply(pd.DataFrame.sample, 5000).reset_index(drop=True).sample(frac=.5).reset_index(drop=True)\n",
    "\n",
    "# build and fit the model\n",
    "model = smf.glm(\n",
    "    formula=formula, \n",
    "    data=df_,\n",
    "    family=sm.families.Binomial()\n",
    ").fit()\n",
    "\n",
    "# print the summary\n",
    "print(model.summary())\n",
    "\n",
    "# plot the distributions together\n",
    "fig = ff.create_distplot(\n",
    "    [\n",
    "        df.sample(10000).loc[df.arson == 1].sod,\n",
    "        df.sample(10000).loc[df.arson == 0].sod\n",
    "    ],\n",
    "    [\n",
    "        'Arson', \n",
    "        'Everything Else'\n",
    "    ], \n",
    "    bin_size=1000,\n",
    "    show_rug=False,\n",
    "    colors=[\n",
    "        '#002450',\n",
    "        '#588C07'\n",
    "    ]\n",
    ")\n",
    "fig['layout'].update(\n",
    "    title='The sod Distriubutions Are Hardly Discernable',\n",
    "    xaxis=dict(\n",
    "        title='Second of Day',\n",
    "    )\n",
    ")\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic regression and gradient boosted models clearly do not agree. The GBM gave a high importance to the second of the day for which the fire was discovered but, the Logistic regression did not agree. It must be noted that the regression model's deviance is quite high indicating a poor fit.\n",
    "\n",
    "The federal lands question is quite clear. Arsonists are definitely less inclined to burn federal lands. This is a good thing for our tax dollars! This is also a good thing to note if the federal government starts taking more direct action to engineer the forests for Co2 extraction from the atmosphere. This is something California is currently passing into law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An important limitation to mention is the nature of the wildfires dataset. It was aggregated over 25 years of varying federal and local agencies. This becomes evident when taking a look at the map at the beginning of the notebook. Kentucky seemed to place heavy importance on reporting campfire caused incidents. You can see this by the distinct outline of a unique color around the state. Other states of interest are Louisiana and New York. The majority of Louisiana fires had missing or underfined classification labels. New York stands out for for the extreme level of reporting. It's very easy to pick the state of the scatter plot even though no boundaries were drawn. In contrast, the Southern border of Virginia is starkly depicted against the dense reporting of North Carolina.\n",
    "\n",
    "An additional limitation is the bias in weather station location placement. Roughly 25% of wildfires occurred more than 55km from the nearest station. This may not cause a problem with our dataset given how insignificant the majority of our weather columns were in contributing to model inference. But, it is something that should be noted for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T00:02:31.494035Z",
     "start_time": "2018-12-07T00:02:31.392609Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  What are the most important indicators to consider when determining the cause of a wildfire?\n",
    "\n",
    "The answer to this question is somewhat anticlimatic. I expected to learn a great deal from joining the vegetation and soil content data. I'm dissapointed that we were unable to take advantage of the data. In the future, I plan on using the Google Earth Engine for any environmental related products that I produce. The engine demanded too steep of a learning curve for me to utilize in this project but I look forward to learning it. Despite the setback we still gathered some useful information. \n",
    "\n",
    "As it turns out, weather doesn't correlate very well to the cause of a fire. It happens that more lightning occurs with both drier climates and tends to start fires more easily. None of that should surprise anyone. \n",
    "\n",
    "Some interesting seasonality showed itself through the second of year feature. I did not expect arsonists to work more in the spring time nor children to enjoy burning in the Spring.\n",
    "\n",
    "The number of nearby fires also turned out to be a good indicator for predicting the cause.\n",
    "\n",
    "#### Can a reliable model be built to assist investigators in determining the cause of a wildfire?\n",
    "\n",
    "No, not really. With the features we have right now I wouldn't say that our model was very reliable for predicting the cause of a fire. Predicting only arson caused fires was decently successful with 87% accuracy for cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T05:10:14.435064Z",
     "start_time": "2018-12-09T05:10:14.425110Z"
    }
   },
   "source": [
    "[1] Short, Karen C. 2017. Spatial wildfire occurrence data for the United States, 1992-2015 [FPA_FOD_20170508]. 4th Edition. Fort Collins, CO: Forest Service Research Data Archive. https://doi.org/10.2737/RDS-2013-0009.4\n",
    "\n",
    "[2] Gillett, N. P., & Weaver, A. J. (2004).Detecting the effect of climate change on Canadian forest fires. AGU100 Advancing Earth and Space Science, 31(18). Retrieved from https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2004GL020876\n",
    "\n",
    "[3] Forests and climate change. (2017, November 10). Retrieved November 21, 2018, from https://www.iucn.org/resources/issues-briefs/forests-and-climate-change\n",
    "\n",
    "[4] The Paris Agreement | UNFCCC. (n.d.). Retrieved November 21, 2018, from https://unfccc.int/process-and-meetings/the-paris-agreement/the-paris-agreement\n",
    "\n",
    "[5] Forests provide a critical short-term solution to climate change. (2018, June 22). Retrieved November 21, 2018, from http://www.unenvironment.org/news-and-stories/story/forests-provide-critical-short-term-solution-climate-change\n",
    "\n",
    "[6] Facts + Statistics: Wildfires | III. (n.d.). Retrieved November 21, 2018, from https://www.iii.org/fact-statistic/facts-statistics-wildfires\n",
    "\n",
    "[7] NPP Multi-Biome: NPP and Driver Data for Ecosystem Model-data Intercomparison, R2. (n.d.). Retrieved November 21, 2018, from https://daac.ornl.gov/NPP/guides/NPP_EMDI.html\n",
    "\n",
    "[8] Olson, R.J., J.M.O. Scurlock, S.D. Prince, D.L. Zheng, and K.R. Johnson (eds.). 2013. NPP Multi-Biome: NPP and Driver Data for Ecosystem Model-Data Intercomparison, R2. Data set. Available on-line http://daac.ornl.gov from Oak Ridge National Laboratory Distributed Active Archive Center, Oak Ridge, Tennessee, USA. doi:10.3334/ORNLDAAC/615\n",
    "\n",
    "[9] 2010-01-30: Surface Summary of Day, GSOD - Datafedwiki. https://data.nodc.noaa.gov/cgi-bin/iso?id=gov.noaa.ncdc:C00516\n",
    "\n",
    "[10] About Us - ORNL DAAC. https://daac.ornl.gov/about/\n",
    "\n",
    "### Other references\n",
    "[Azure Notebooks FAQ](https://notebooks.azure.com/faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hcds-final]",
   "language": "python",
   "name": "conda-env-hcds-final-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "563.333px",
    "left": "640.333px",
    "right": "20px",
    "top": "211px",
    "width": "621.323px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
